{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CycleGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from functools import partial"
      ],
      "metadata": {
        "id": "V2eb4oY_WDzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PixelDiscriminator(nn.Module):\n",
        "  \"\"\"Defines a 1x1 PatchGAN discrimator (pixelGAN)\"\"\"\n",
        "  def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      input_nc(int)  -- the number of channels in input images\n",
        "      ndf(int)   -- the number of filters in the last conv layer\n",
        "      norm_layer  -- normalization layer\n",
        "    \"\"\"\n",
        "    super(PixelDiscriminator, self).__init__()\n",
        "    if type(norm_layer) == functools.partial:\n",
        "      use_bias = norm_layer.func == nn.InstanceNorm2d   # https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html\n",
        "    else:\n",
        "      use_bias = norm_layer == nn.InstanceNorm2d\n",
        "    self.net = [\n",
        "                nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),   # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "                nn.LeakyReLU(0.2, True),\n",
        "                nn.Conv2d(ndf, ndf*2, kernel_size=1, stride=1, padding=0, bias=use_bias),\n",
        "                norm_layer(ndf*2),\n",
        "                nn.LeakyReLU(0.2, True),\n",
        "                nn.Conv2d(ndf*2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]\n",
        "    self.net = nn.Sequential(*self.net)\n",
        "  def forward(self, input):\n",
        "    return self.net(input)      # standard forward!"
      ],
      "metadata": {
        "id": "J8p4nqqBUqS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NLayerDiscriminator(nn.Module):\n",
        "  \"\"\"Define a PatchGAN discriminator\"\"\"\n",
        "  def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      input_nc(int)\n",
        "      ndf (int)\n",
        "      n_layers (int)  -- the number of conv layers in the discriminator\n",
        "      norm_layer\n",
        "    \"\"\"\n",
        "    super(NLayerDiscriminator, self).__init__()\n",
        "    if type(norm_layer) == functools.partial:\n",
        "      use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "    else:\n",
        "      use_bias = norm_layer == nn.InstanceNorm2d\n",
        "    \n",
        "    kw = 4\n",
        "    padw = 1\n",
        "    sequence = [\n",
        "                nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n",
        "                nn.LeakyReLU(0.2, True)]\n",
        "    nf_mult = 1\n",
        "    nf_mult_prev = 1\n",
        "    for n in range(1, n_layers):\n",
        "      nf_mult_prev = nf.nf_mult\n",
        "      nf_mult = min(2 ** n, 8)\n",
        "      seqeunce += [\n",
        "                   nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
        "                   norm_layer(ndf * nf_nult),\n",
        "                   nn.LeakyReLU(0.2, True)\n",
        "      ]\n",
        "      nf_mult_prev = nf_mult_prev\n",
        "      nf_mult = min (2 ** n_layers, 8)\n",
        "      sequence += [\n",
        "                   nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
        "                   norm_layer(ndf * nf_mult),\n",
        "                   nn.LeakyReLU(0.2, True)\n",
        "      ]\n",
        "\n",
        "      sequence += [\n",
        "                   nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)\n",
        "      ]\n",
        "      self.model = nn.Sequential(*sequence)\n",
        "    \n",
        "    def forward(self, input):\n",
        "      return self.model(input)"
      ],
      "metadata": {
        "id": "cz8aCtWbW2wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnetGenerator(nn.Module):\n",
        "  \"\"\"Create a Unet-based generator\"\"\"\n",
        "  def __init__(self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "    \"\"\"Construct a Unet generator from the innermost layer to the outermost layer\n",
        "    Parameters:\n",
        "      input_nc (int)\n",
        "      output_nc (int)\n",
        "      num_downs (int)\n",
        "      ngf (int)\n",
        "      norm_layer\n",
        "    \"\"\"\n",
        "    super(UnetGenerator, self).__init__()\n",
        "    unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)\n",
        "    for i in range(num_downs - 5):\n",
        "      unet_blovk = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "    unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "    unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "    unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "    self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)\n",
        "\n",
        "  def forward(self, input):\n",
        "    return self.model(input)"
      ],
      "metadata": {
        "id": "3Vy2jzX8Zahc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResnetGenerator(nn.Module):\n",
        "  \"\"\"Resnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations.\"\"\"\n",
        "  def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type='reflect'):\n",
        "    \"\"\"Construct a Resnet-based generator\n",
        "    \"\"\"\n",
        "    assert(n_blocks >= 0)\n",
        "    super(ResnetGenerator, self).__init__()\n",
        "    if type(norm_layer) == functools.partial:\n",
        "      use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "    else:\n",
        "      use_bias = norm_layer == nn.InstanceNorm2d\n",
        "    \n",
        "    model = [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n",
        "            norm_layer(ngf),\n",
        "            nn.ReLU(True)]\n",
        "    n_downsampling = 2\n",
        "    for i in range(n_downsampling):\n",
        "      mult = 2 ** i\n",
        "      model += [\n",
        "                nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n",
        "                norm_layer(ngf * mult * 2),\n",
        "                nn.ReLU(True)]\n",
        "    mult = 2 ** n_downsampling\n",
        "    for i in range(n_blocks):\n",
        "      model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n",
        "    for i in range(n_downsampling):\n",
        "      mult = 2 ** (n_downsampling - i)\n",
        "      model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1, bias=use_bias), norm_layer(int(ngf * mult / 2)), nn.ReLU(True)]\n",
        "    \n",
        "    model += [nn.ReflectionPad2d(3)]\n",
        "    model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
        "    model += [nn.Tanh()]\n",
        "\n",
        "    self.model = nn.Sequential(*model)\n",
        "\n",
        "  def forward(self, input):\n",
        "    return self.model(input)"
      ],
      "metadata": {
        "id": "Ym3VZBDCa1z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://cvnote.ddlee.cc/2019/09/02/cyclegan-pytorch-github\n",
        "class CycleGANModel(nn.Module):\n",
        "  \"class CycleGANModel: CycleGAN for learning image-to-image translation without paired data.\"\n",
        "  def forward(self):\n",
        "    self.fake_B = self.netG_A(self.real_A)\n",
        "    self.rec_A = self.netG_B(self.fake_B)\n",
        "    self.fake_A = self.netG_B(self.real_B)\n",
        "    self.rec_B = self.netG_Z(self.fake_A)\n",
        "  def backward_D_basic(self, netD, real, fake):\n",
        "    \"\"\"Calculate GAN loss for the discriminator\"\"\"\n",
        "    # real\n",
        "    pred_real = netD(real)\n",
        "    loss_D_real = self.criterionGAN(pred_real, True)\n",
        "    # fake\n",
        "    pred_fake = netD(fake.detach())\n",
        "    loss_D_fake = self.criterionGAN(pred_fake, False)\n",
        "    # Combined loss and calculate gradients\n",
        "    loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
        "    loss_D.backward()\n",
        "    return loss_D\n",
        "  def backward_D_A(self):\n",
        "    \"\"\"Calculate GAN loss for the discriminator D_A\"\"\"\n",
        "    fake_B = self.fake_B_pool.query(self.fake_B)\n",
        "    self.loss_D_A = self.backward_D_basic(self.netD_A, self.real_B, fake_B)\n",
        "  def backward_D_B(self):\n",
        "    \"\"\"Calculate GAN loss for the discriminator D_B\"\"\"\n",
        "    fake_A = self.fake_A_pool.query(self.fake_A)\n",
        "    self.loss_D_B = self.backward_D_basic(self.netD_B, self.real_A, fake_A)\n",
        "  def backward_G(self):\n",
        "    \"\"\"Calculate GAN loss for the discriminator G_A and G_B\"\"\"\n",
        "    lambda_idt = self.opt.lambda_identity\n",
        "    lambda_A = self.opt.lambda_A\n",
        "    lambda_B = self.opt.lambda_B\n",
        "    # identity loss\n",
        "    if lambda_idt > 0:\n",
        "      # G_A should be identity if real_B is fel: || G_A(B) - B ||\n",
        "      self.idt_A = idt.netG_A(self.real_B)\n",
        "      self.loss_idt_A = self.criterionIdt(self.idt_A, self.real_B) * lambda_B * lambda_idt\n",
        "      # G_B should be identity if real_A is fel: || G_B(A) - A ||\n",
        "      self.idt_B = self.netG_B(self.real_A)\n",
        "      self.loss_idt_B = self.criterionIdt(self.idt_B, self.real_A) * lambda_A * lambda_idt\n",
        "    else:\n",
        "      self.loss_idt_A = 0\n",
        "      self.loss_idt_B = 0\n",
        "    \n",
        "    self.loss_G_A = self.criterionGAN(self.netD_A(self.fake_B), True) # GAN loss D_A(G_A(A))\n",
        "    self.loss_G_B = self.criterionGAN(self.netD_B(self.fake_A), True) # GAN loss D_B(G_B(B))\n",
        "    self.loss_cycle_A = self.criterionCycle(self.rec_A, self.real_A) * lambda_A # forward cycle loss || G_B(G(A)) - A ||\n",
        "    self.loss_cycle_B = self.criterionCycle(Self.rec_B, self.real_B) * lambda_B # backward cycle loss || G_A(G_B(B)) - B ||\n",
        "    self.loss_G = self.loss_G_A + self.loss_G_B + self.loss_cycle_A + self.loss_cycle_B + self.loss_idt_A + self.loss_idt_B # combined loss and calculate gradients\n",
        "    self.loss_G.backward()"
      ],
      "metadata": {
        "id": "YZSKu87AKxz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "q8MWq-_Lh54H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}