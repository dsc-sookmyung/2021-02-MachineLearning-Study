# 심층 신경망 네트워크

## 더 많은 층의 심층 신경망

- 얼마나 깊은 신경망을 사용해야 하는지 미리 정확하게 예측하기 어려움
- 표기법
    - L : 네트워크 층의 수
    - n^[l] : l층에 있는 유닛 개수
    - a^[l] : l층에서의 활성값
    - a^[0] : 입력 특징 (X)
    - a^[L] : 예측된 출력값 (y^)

## 왜 심층 신경망이 더 많은 특징을 잡아낼 수 있을까?

- 직관 1: 네트워크가 더 깊어 질 수록, 더 많은 특징을 잡아낼 수가 있음. 낮은 층에서는 간단한 특징을 찾아내고, 깊은 층에서는 탐지된 간단한 것들을 함께 모아 복잡한 특징을 찾아낼 수 있음.
- 직관 2: 순환 이론에서 따르면, 상대적으로 은닉층의 개수가 작지만 깊은 심층 신경망에서 계산할 수 있는 함수가 있음.
    - but, 얕은 네트워크로 같은 함수를 계산하려고 하면, 즉 충분한 은닉층이 없다면 기하급수적으로 많은 은닉 유닛이 계산에 필요하게 됨.
    - 순환 이론: 로직 게이트의 서로 다른 게이트에서 어떤 종류의 함수를 계산할 수 있을지에 관한 것.

## 심층 신경망 네트워크 구성하기

- *l* 번째 층에서 정방향 전파는 이전 층의 활성화 값인 *a^*[*l*−1]을 입력으로 받고, 다음 층으로 *a^*[*l*] 값을 출력으로 나오게 함.
    - 이때 선형결합된 값인 *z^*[*l*] 와 변수 *W^*[*l*],*b^*[*l*] 값도 캐시로 저장
- *l* 번째 층에서 역방향 전파는 *da^*[*l*] 을 입력으로 받고, *da^*[*l-1*] 를 출력.
    - 이때 업데이트를 위한 *dW^*[*l*] 와 *db^*[*l*] 도 함께 출력
    - 이들을 계산하기 위해서 전방향 함수때 저장해두었던 캐시를 사용
    

## 변수와 하이퍼파라미터

- 하이퍼파라미터 : 학습 알고리즘에 알려줘야할 것 → 매개변수 W와 b를 통제
    - 학습률
    - 수행하는 경사 하강법의 반복 횟수
    - 은닉층의 개수 (L)
    - 은닉유닛의 갯수
    - 활성화 함수의 선택 (ReLU, TanH, Sigmoid 등)
    - 모멘텀항
    - 미니배치 크기
- 하이퍼 파라미터는 계속 시도하며 적합한 값을 찾아야함!