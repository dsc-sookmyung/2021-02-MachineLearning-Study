# 신경망과 로지스틱회귀

## 이진 분류 (Binary Classification)

- 로지스틱 회귀 : 이진 분류를 위한 알고리즘

### 이진 분류에 대한 문제

- ex) 고양이 사진이 입력으로 주어졌을 때, 두 범주 중 어떻게 분류했는지 출력하길 원함
- 고양이 사진 → 1(cat), 0(non cat) 
⇒ y로 출력 레이블
- 사진이 어떻게 컴퓨터에 저장되는가?
    - Red, Green, Blue 세 개의 행렬을 따로 저장함.
    - 모든 픽셀값을 하나의 벡터로 펼치기 위해 주어진 사진에 대한 특성 벡터 x를 정의
    - 픽셀값 전부 픽셀 강도값으로 나타냄.
        - ex) 64*64 일 경우 벡터 x의 전체 차원은 64 * 64 * 3 = 12,288
        - n = n_x = 12,288
        

### 이진 분류의 목표

- 입력된 사진을 나타내는 특성 벡터 x를 가지고 그에 대한 레이블 y가 1, 0인지 예측할 수 있는 분류기를 학습하는 것!

### 표기법 소개

<img width="500" alt="스크린샷_2021-10-02_오전_12 29 56" src="https://user-images.githubusercontent.com/66219968/135964399-80b54ec3-b58a-4607-9530-67d661da2899.png">

- 단 하나의 훈련 샘플을 한 쌍 (x, y)로 표시할 때, x는 n_x차원 상의 특성 벡터이고 레이블 y는 0 또는 1
- 훈련세트는 m개의 훈련샘플을 포함하고 (x^(1), y^(1)), (x^(2),y^(2)), (x^(3), y^(3)) ... (x^(m), y^(m))
    - 훈련 샘플의 개수를 m으로 표시, 강조하기 위해 m_train 라고 표기하기도 함.
    - 테스트 세트에 대해서는 테스트 세트의 개수를 m_test로 표시
- m이 훈련 샘플의 개수일 때 X는 m개의 열과 n_x개의 행들로 이루어짐.
    - X.shape = (n_x, m)
- 레이블 Y
    - 신경망의 구현을 더 쉽게하기 위해 y의 값들을 열로 놓는 것이 편리
    - Y = [y^(1), y^(2), ..., y^(m)] 로 이루어진 1xm 행렬
    - Y.shape = (1,m)

## 로지스틱 회귀

- 로지스틱 회귀란 답이 0 또는 1로 정해져있는 이진 분류 문제에 사용되는 알고리즘
- x (입력 특성), y (주어진 입력특성 x에 해당하는 실제 값), y^ (y의 예측값)을 의미
y^ 는 y가 1일 확률을 의미하며, 0≤ y^ ≤ 1 사이의 값을 가져야 함.
- x는 n_x 차원 상의 벡터, 로지스틱 회귀의 파라미터 w와 b는 각각 n_x차원 상의 벡터와 실수.
- 입력 x와 파라미터 w와 b가 주어졌을 때 y의 예측값을 출력하는 법
    - y^ = W^T X + b
        
        ⇒ 이진 분류를 위한 좋은 알고리즘은 아님
        ⇒  y의 예측값은 y가 1일 확률이기 때문에 항상 0과 1 사이어야하기 때문
        
    - 시그모이드 함수를 통해 0과 1 사이의 값으로 변환
        - z는 W^T X + b의 값을 의미
        - z가 실수 일 때,z의 시그모이드는 1/(1 + e^(-z))
            - 만약 z가 아주 크다면, e^(-z)가 0으로 수렴
                - z의 시그모이드는 1과 0에 가까운 숫자의 합을 1에서 나누었기 때문에 1에 가까움.
                - 즉, z가 아주 크면 z의 시그모이드가 1에 수렴
            - z가 아주 작거나, 아주 큰 음수일 경우, z의 시그모이드는 1/(1 + e^(-z))이고, e^(-z)가 아주 큰 수
                - 1과 엄청 큰 수를 1에서 나누었기 때문에 0과 가까워짐.
                - z가 큰 음수일 때, z의 시그모이드가 0에 수렴
        
        <img width="500" alt="스크린샷_2021-10-03_오후_5 21 32" src="https://user-images.githubusercontent.com/66219968/135964405-ca28c706-3b8a-4a9f-938e-ac0491dbbd2e.png">
        
- 로지스틱 회귀를 구현할 때 y가 1일 확률을 잘 예측하도록 파라미터 w와 b를 학습해야함.

## 로지스틱 회귀의 비용함수

<img width="500" alt="스크린샷_2021-10-03_오후_5 35 55" src="https://user-images.githubusercontent.com/66219968/135964407-019809c4-da8e-4368-b843-0c4018407e26.png">

- 로지스틱 회귀 모델의 매개변수 w와 b를 학습하려면 비용함수를 정의해야함

### 손실 함수

- 하나의 입력특성(x)에 대한 실제값(y)과 예측값(*y*^) 의 오차를 계산하는 함수
    
    (= 단일 훈련 샘플이 얼마나 잘 작동하는지 측정)
    
- L(y^,y) = 1/2(y^-y)^2 식으로 사용하지만 로지스틱 회귀에서 이러한 손실 함수를 사용하면 지역 최소값에 빠질 수 있기 때문에 사용하지 않음.
- 로지스틱 회귀에서 사용하는 손실 함수
L(y^, y) = -(y * log y^) + (1-y) * log(1-y^)
    - 만약 제곱 오차를 사용하면, 그 오차를 최소화하려 할 것
    - y = 1 일 때, L(y^, y) = -log y^
        - -log y^ 값이 최대한 커지길 원함 → y^이 최대한 커야함.
        - but, y^은 시그모이드 함수값이기 때문에 1보다 클 수 없음.
        - 따라서, y^이 1보다 클 수 없으므로 1에 수렴하길 원함.
    - y = 0 일 때, 손실 함수는 -log(1-y^)
        - 손실 함숫값을 줄이기 위해선 -log(1-y^) 이 최대한 커야함. → y^이 최대한 작아야함.
        - y^는 시그모이드 함수값이기 때문에 0과 1 사이의 값이므로, y = 0 일 때, 손실 함수는 y의 예측값이 0에 수렴하도록 매개변수들을 조정
    - y가 1일 때, y의 예측값이 크고, y가 0일 때 y의 예측값이 작은 성질을 가지고 있는 함수들은 많음.
        - 위는 비공식적인 검증
    - 손실 함수는 훈련 샘플 하나에 관하여 정의돼서 하나가 얼마나 잘 예측 되었는지 측정해줌
    

### 비용함수

- 훈련 세트 전체에 대해 얼마나 잘 추측되었는지 측정해주는 함수
    
    (= 매개변수 w, b가 전체를 얼마나 잘 예측하는지 측정)
    
- J(w,b) = 손실 함수를 각각의 훈련 샘플에 적용한 값의 합들의 평균

<img width="500" alt="스크린샷_2021-10-03_오후_5 49 45" src="https://user-images.githubusercontent.com/66219968/135964415-fa47adf2-e1b8-4068-9e9f-a2a2aec758f3.png">

- 비용 함수는 매개 변수의 비용처럼 작용한다는 점
- 로지스틱 회귀 모델을 학습하는 것은 손실 함수 J를 최소화해주는 매개 변수들 w와 b를 찾는 것

## 경사하강법

- 비용함수가 전체 데이터셋의 예측이 얼마나 잘 평가되었는지 보는 것이라면,
경사하강법은 이를 가능하게 한 w,b를 찾아내는 방법 중 하나!

<img width="500" alt="스크린샷_2021-10-03_오후_6 00 06" src="https://user-images.githubusercontent.com/66219968/135964418-aab6f63c-2651-4d4b-9e5a-f22d8584f080.png">

- 매개변수 w,b를 알아내기 위해서는 비용함수를 가장 작게 만드는 w,b를 찾아야함.
- 경사 하강법 알고리즘을 이용하여 매개변수 w, b를 학습시키는 법
    - 볼록하지 않은 함수는 지역 최적값이 여러개! → 최적값을 찾을 수 없음.
    - 비용함수가 볼록하다는 사실이 로지스틱 회귀에 위의 비용함수 J를 사용한 가장 큰 이유!
    - 함수의 최소값을 모르기 때문에, 임의의 점을 골라서 시작 → 매개변수에 쓸 좋은 값을 찾기 위해 w,b를 초기화해야함
        - 주로 0으로 설정. 무작위는 잘 안 함.
            
            <img width="600" alt="스크린샷_2021-10-03_오후_6 03 05" src="https://user-images.githubusercontent.com/66219968/135964419-80dca687-d064-4cbb-9a7c-8b87771c716a.png">
            
    - 경사 하강법에서는 초기점에서 시작해 가장 가파른 내리막 방향으로 한 단계 내려감 → 전역 최적값이나 근사치에 도달하게 됨.

<img width="500" alt="스크린샷_2021-10-03_오후_6 12 53" src="https://user-images.githubusercontent.com/66219968/135964423-874058b6-d7d9-40be-95d5-027c24428616.png">

<img width="500" alt="스크린샷_2021-10-03_오후_6 23 45" src="https://user-images.githubusercontent.com/66219968/135964424-03b65111-7023-41bf-81fa-a7bd8d15d488.png">

- 만약 dw >0 이면, 파라미터 w 는 기존의 w 값 보다 작은 방향으로,
만약 dw <0 이면, 파라미터 w 는 기본의 w 값 보다 큰 방향으로 갱신됨.
- 도함수는 함수의 기울기
    - 도함수에 대한 표기
        
        <img width="500" alt="스크린샷_2021-10-03_오후_6 24 38" src="https://user-images.githubusercontent.com/66219968/135964425-46367d70-380e-40b6-be3a-8286d65cf20a.png">
        
    

## 계산 그래프

- 신경망의계산은 정방향 패스 (정방향 전파는 긴경망의 출력값을 계산)
이는 역방향 패스 ,역방향 전파로 이어져 강사나 도함수를 걔산
- 예제) J(a, b, c) = 3(a  + bc) 일때, 함수 J의 계산은 다음의 순서를 따른다.
    
    <img width="500" alt="스크린샷_2021-10-03_오후_7 17 46" src="https://user-images.githubusercontent.com/66219968/135964427-017059c9-42ac-4a7f-85d1-5df563a1cf0a.png">
    
    1. u = bc
    2. v = a+ u
    3. J = 3v
- 미분의 연쇄법칙은 합성함수의 도함수에 대한 공식 → 합성함수를 구성하는 함수의 미분을 곱해서 구할 수 있음.
    - 입력변수 a 를 통해서 출력변수 J 까지 도달 하기 위해서 a→v→J 의 프로세스로 진행
    - 즉, 변수 a 만 보게 된다면, J = J(u(a)) 라는 함성함수가 될 것
    - 합성함수 a로 미분한 값 dJ/da를 구하기 위해 dJ/dv와 dv/da의 곱으로 표현
- 표기법
    - 최종 변수를 Final output var, 미분하려고 하는 변수를 var 라고 정의 한다면,
        
        **d Final output var / d var = d var**
        

## 로지스틱 회귀의 경사하강법

- 로지스틱 회귀의 경사하강법을 위해 필요한 핵심 공식 구현 방법
    - a는 로지스틱 회귀의 출력값, y는 참 값 레이블
        
        <img width="500" alt="스크린샷_2021-10-03_오후_8 00 01" src="https://user-images.githubusercontent.com/66219968/135964428-cfd1a417-abbb-43a2-979d-f69cbcdbe4d1.png">
        
        <img width="500" alt="스크린샷_2021-10-03_오후_8 00 31" src="https://user-images.githubusercontent.com/66219968/135964429-92a30f59-513c-49c3-a9ca-e3e143f84e25.png">
        
    - 손실함수의 도함수를 구하는 것이니 역방향으로 가서 에이에대한 손실함수의 도함수를 계산 → 코드에서는 da로 표시 가능
        
        <img width="500" alt="스크린샷_2021-10-03_오후_8 01 30" src="https://user-images.githubusercontent.com/66219968/135964432-ec26f348-90d0-4f6d-8a08-18113ff9a1a6.png">
        
- m개의 훈련 샘플에 대해
    
    <img width="500" alt="스크린샷_2021-10-03_오후_8 15 55" src="https://user-images.githubusercontent.com/66219968/135964433-941e153c-fe3c-41f3-ac5f-3f3f5c5e5026.png">
