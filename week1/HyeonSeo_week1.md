# Week1 머신러닝 스터디 정리

뉴런
---

주택가격 예측 예제로 6개의 주택 데이터가 있다고 했을 때 주택의 크기를 바탕으로 주택의 가격을 예측하는 함수를 만든다  
-> 하나의 간단한 신경망  
**size X -> 뉴런 -> price Y  
  
여기서 뉴런이 하는 일  
: 주택의 크기를 입력받아 선형함수를 계산하고, 결과값과 0 중 더 큰 값을 주택의 가격으로 예측한다  
  
뉴런 하나를 하나의 레고블록으로 볼 수 있다  
-> 더 많은 레고블록을 쌓음으로 더 큰 신경망을 만들 수 있다  

---
신경망
---

주택 가격 에측으로   
**입력값 X**              ->         **은닉유닛**        ->           **결과값 Y**  
집의 크기(size)                   가족의 크기(family size)    
침실의 개수(bedrooms)                                              집의 가격  
우편번호(zipcode)                 걸어다니기 좋은 동네  
동네의 부(wealth)                 혹은 명문 학교의 여부      

신경망 : 입력값을 바탕으로 여러개의 은닉유닛을 가지고, 이를 종합하여 결과값을 알아낸다  
         은닉 유닛은 4개의 입려을 다 받는다. -> 입력값(X)와 서로 긴밀하게 연결되어 있다고 할 수 있다  
         충분한 양의 XY훈련예제를 준다면 X를 Y로 연결하는 함수가 뛰어나질 것이다.  
   
신경망을 통해 만들어진 경제적 가치들은 머신러닝의 한 종류인 지도학습에 의해 만들어졌다.  
**지도학습 : 입력 X와 출력 Y에 매핑되는 함수를 학습하려 한다**  
ex) 광고 정보와 유저 정보를 바탕으로 온라인 광고를 수행       ->      표준 신경망 구조  
    서로 다른 이미지들을 태깅                                ->      표준 신경망 구조  
    음성인식                                                 ->      CNN(합성곱 신경망)  
    기계번역                                                 ->      RNN(순환신경망)  
    자연어 처리                                              ->      RNN  
    앞 차에 대한 정보와 레이저 정보를 통한 자율주행           ->      더 복잡한 하이브리드  
     
적절한 XY를 통해 지도학습, 서로 다른 적절한 응용분야에 적용  





데이터의 종류   
**구조적 데이터 : 데이터베이스로 표현된 데이터**  
                    ex) 주택가격(크기, 침실의 수 등), 광고 클릭(사용자 나이, 광고에 대한 정보 등)  
**비구조적 데이터 : 음성파일, 이미지, 텍스트 등 픽셀값이나 단어로 이루어진 데이터**  
-> 구조적 데이터보다 비구조적 데이터가 컴퓨터가 작업하기 훨씬 어렵다  
-> 딥러닝 덕분에 비구조적 데이터 분석이 많이 발전했다  
그러나 발생하는 경제적인 이익은 보통 구조적 데이터에서 많이 발생한다. 방대한 데이터를 기반으로 정확한 예측을 하는 경우 딥러닝이 유리하다  
  
딥러닝이 급부상한 이유  
---
  데이터의 양이 많아질수록 전통적인 학습 알고리즘의 성능은 정체기에 이른다  
  20년간 사회가 변화하면서 꽤 많은 데이터를 가지게 되었는데, 전통적인 학습 알고리즘이 효과적으로 처리할 수 있는 양 이상의 데이터가 쌓이게 되었다  
  많지 않은 데이터를 처리하는 부분에서 전통적인 학습 알고리즘은 딥러닝 학습 알고리즘보다 더 효과적일 수 있으나  
  데이터가 많아질수록 신경망이 큰 딥러닝 학습 알고리즘과 전통적인 학습 알고리즘의 성능 차이가 벌어지게 되었다  
    
**딥러닝의 아주 좋은 성능을 위해 필요한 두가지 요소**  
1. 많은 양의 데이터를 이용하기 위한 충분히 큰 신경망  
2. 많은 데이터  
신경망의 크기 = 많은 은닉유닛, 많은 연결, 많은 파라미터, 데이터의 규모   
성능을 올릴 때 가장 신뢰할 수 있는 방법 : 더 큰 신경 훈련망 또는 더 많은 데이터  
  
데이터의 규모가 딥러닝을 부상하게 만들었다  
  
딥러닝의 발전에는 방대한 양의 데이터, 컴퓨터의 빠른 계산능력, 그리고 알고리즘이 뒷받침한다  
초창기 딥러닝의 문제는 데이터와 계산의 규모에 있었다  
  기존의 시그모이드 함수는 양 끝단에서 미분값이 0과 가까워질수록 학습이 느려진다.  
  이를 ReLu함수를 이용하는 알고리즘으로 변화시키자 입력값이 양수인경우, 경사가 모두 1로 같고, 경사가 서서히 0으로 수렴할 확률이 줄어든다  
  즉, ReLu함수가 계산능력을 더 빠르게 만들었다  
  이는 빠른 계산능력에 영향을 미쳤다  
  빠른 계산이 중요한 이유는 많은 경우 신경망을 학습시키는 과정이 반복적이기 때문이다  
  **신경망을 학습시키는 과정 : 아이디어 -> 코드작성 -> 실험 -> 결과 -> 신경망 수정 -> 반복**  
  계산이 오래걸리면 위 사이클이 도는 시간도 오래걸리고, 결국 생산생에도 영향을 미친다  
  빠른 계산시간은 실험의 결과를 얻는 시간을 빠르게 만들었고, 이는 신경망을 사용하는 사람과 딥러닝 연구자들에게 아이디어를 더 빨리 연구할 수 있도록 도움을 주었다  


---
활성화함수
---
* 시그모이드함수  
        주로 이진분류에서 사용  
        예측값은 0과 1사이의 값  
        평균값은 1/2  
        출력레이어에서 많이 사용한다  
        양 극단으로 갈수록 도함수가 0에 수렴한다 -> 경사하강법과 학습 속도가 느려진다  
        
* tanh함수  
        tanh(z) = (e^z - e^-z)/(e^z + e^-z)
        평균값이 0이기 때문에 대부분의 경우 시그모이드 함수보다 좋은 결과를 만들어낸다
        양 극단으로 갈수록 도함수가 0에 수렴한다 -> 경사하강법과 학습 속도가 느려진다
        
* ReLU함수  
        a = max(0, z)  
        대부부의 경우 좋은 결과를 만들어낸다  
        대부분의 z에 대해 도함수가 0이 아니기 때문에 시그모이드함수와 tanh함수보다 유리하다  
        그러나 0미만의 값에서 도함수가 0이 된다. 대부분의 경우 z는 0보다 크기 때문에 문제는 없다  
        미분 불가능한 점인 z=0에서 기울기를 어떻게 보든 상관 없다
        
* leakyReLU함수  
        a = max(0.01z, z)  
        0.01z도 알고리즘의 변수로 활용할 수 있다  
        ReLU함수가 0미만의 값에서 도함수가 0이되는 문제를 해결하기 위해 약간의 기울기를 부여했다  
        
**주로 은닉 레이어에서는 ReLU를 제일 많이 사용하고, 가끔 tanh를 사용한다  
**출력 레이어에서는 주로 시그모이드를 사용하고, 실수인 경우 선형함수도 사용할 수 있다


---
변수 랜덤 초기화
---
모든 변수(파라미터)를 0으로 초기화하는 경우 **완전대칭문제**가 발생한다

W^[1] = [[0 0][0 0]], b=[[0][0]]으로 초기화하는 경우
a^[1]1 = a^[1]2
n^[1]1과 n^[1]2 은닉 유닛은 모두 서로 같은 함수를 계산한다
그 결과 두개의 활성이 같은 것이 되고, 가중치의 결과값도 항상 같은 결과를 만들어낸다(완전대칭문제)

1층부터 두 유닛이 같은 함수를 계산하는 것으로 시작하여 출력 유닛에 항상 같은 영향을 미치게 되고, 첫번째 반복 이후에 항상 같은 상태가 계속해서 반복된다
그 결과 두 은닉유닛은 실제로 한 개의 은닉유닛으로 볼 수 있다

이를 해결하기 위해 랜덤 초기화를 해야 한다
**랜덤초기화 하는 방법**





---   
##신경망과 로지스틱회귀
---

이진분류 : 그렇다(1), 아니다(0) 2개로 분류하는 것  
-> 로지스틱회귀 : 이진분류를 위한 알고리즘  
ex)사진을 입력했을 때 그 사진이 고양이 사진인지(1) 아닌지(0)  
입력특성 X가 주어졌을 경우 y가 1일 확률에 대한 예측값을 y hat으로 표현한다  
파라미터로 n_x차원의 벡터 w, b를 갖는다  

로지스틱회귀를 사용하는 목표는 **실제값과 가까운 예측값**을 구하는 것  
손실함수(에러함수) : 알고리즘이 출력한 y와 실제y 사이의 제곱 오차의 반으로 계산할 수 있지만 로지스틱 회귀에서는 잘 사용하지 않는다

  
시그모이드 함수s(z) = 1/(1+e^-z)  
z가 매우 크다면 1/1에 수렴, z가 매우 작다면 0으로 수렴한다  
  
매개변수 w와 b학습하려면 비용함수를 정의해야 한다
y의 예측값는 w^t * x+b 의 시그모이드



---
##파이썬과 벡터화
---


---
##얕은 신경망 네트워크
---
신경망 : 


---
##심층 신경망 네트워크
---




밑줄 : u <u>밑줄</u>
이텔릭체는 *별표*또는 _언더바_
두껍게는 **별표두개**또는 __언더바두개__
취소선은 ~~물결두개~~
목록
1. 순서존재
2. 순서
  -순서없음 마이너스
  *별
  +더하기
  
수평선 
---
마이너스3개
***
별 3개
___
언더바3개
줄바꿈 띄어쓰기 2개 또는 <br태그



---
느낀점
---
행렬을 배워야 할 것 
