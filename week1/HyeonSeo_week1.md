# Week1 머신러닝 스터디 정리

뉴런
---

주택가격 예측 예제로 6개의 주택 데이터가 있다고 했을 때 주택의 크기를 바탕으로 주택의 가격을 예측하는 함수를 만든다  
-> 하나의 간단한 신경망  
**size X -> 뉴런 -> price Y  
  
여기서 뉴런이 하는 일  
: 주택의 크기를 입력받아 선형함수를 계산하고, 결과값과 0 중 더 큰 값을 주택의 가격으로 예측한다  
  
뉴런 하나를 하나의 레고블록으로 볼 수 있다  
-> 더 많은 레고블록을 쌓음으로 더 큰 신경망을 만들 수 있다  

주택 가격 에측으로   
**입력값 X**              ->         **은닉유닛**        ->           **결과값 Y**  
집의 크기(size)                   가족의 크기(family size)    
침실의 개수(bedrooms)                                              집의 가격  
우편번호(zipcode)                 걸어다니기 좋은 동네  
동네의 부(wealth)                 혹은 명문 학교의 여부      

로지스틱회귀
---
이진분류 : 그렇다(1), 아니다(0) 2개로 분류하는 것  
-> 로지스틱회귀 : 이진분류를 위한 알고리즘  
ex)사진을 입력했을 때 그 사진이 고양이 사진인지(1) 아닌지(0)  
입력특성 X가 주어졌을 경우 y가 1일 확률에 대한 예측값을 y hat으로 표현한다  
파라미터로 n_x차원의 벡터 w, b를 갖는다  

로지스틱회귀의 예측값은 0과 1사이이기 때문에 주로 시그모이드함수를 사용한다
시그모이드 함수는 양 극단으로 갈수록 0과 1에 수렴하는 곡선그래프로, 식은 s(z) = 1/(1+e^-z)이다

로지스틱회귀를 사용하는 목표는 **실제값과 가까운 예측값**을 구하는 것  

손실함수L(y hat, y) : 단일 훈련 샘플이 얼마나 잘 작동하는지 측정하는 함수
                      -(ylog(y hat) + (1-y)log(1-y hat))
                      y = 0 -> L(y hat, y) = -log(1 - y hat)가 0에 가까워지도록 y hat은 0에 수렴하게 된다
                      y = 1 -> L(y hat, y) = -log(y hat)가 0에 가까워지도록 y hat은 1에 수렴한다
                      
                      하나의 입력특성x에 대해 예측값(y hat)과 실제값(y)사이의 오차를 계산하는 함수
                      훈련샘플 하나에 관해 결정되고, 그 샘플이 얼마나 잘 예측되었는지를 보여준다
                      보통 손실함수는 1/2(y hat - y)^2으로 계산하지만, 로지스틱회귀에서는 최적화 문제때문에 이 함수를 잘 사용하지 않는다

비용함수J(w, b) : 매개변수 w, b가 훈련세트 전체를 얼마나 잘 예측하지는지를 측정하는 변수
                  비용함수 J를 가장 작게 만드는 w, b를 찾아야 하고, 비용함수는 볼록해야 한다 -> 경사하강법으로 전역 최적값을 찾기 위함
                  -1/m * S(y^(i) log(y^(i) hat) + (1 - y^(i)) log(1-y^(i) hat)) 
                  손실함수 각각의 훈련샘플에 적용한 값들의 합의 평균
                  매개변수의 비용처럼 적용
                  로지스틱회귀모델의 학습은 비용함수 J를 최소화하는 w와 b를 찾는 것으로 볼 수 있다
                  
                  y hat = s(w^Tx + b)
                  s(z) = 1/(1+e^-z)
                  y = 1 -> P(Y|X) = y hat
                  y = 0 -> P(Y|X) = 1 - y hat
                  => P(Y|X) = y hat (1 - y hat)
                  logP(Y|X) 최대화 = P(Y|X) 최대화
                  => log(y hat)(1 - y hat)^(1-y) = ylog(y hat) + (1-y) log(1 - y hat) = 1 - y hat
                
경사하강법
---
**a(알파) : 학습률 : 경사하강법을 반복할 때 한 단계의 크기를 결정한다**
경사하강법은 w, b훈련세트를 학습시키는 방법이다
비용함수에서 w, b를 초기화한 후(보통 0으로 초기화) 가장 가파른 내리막 방향으로 이동해 전역 최적값을 찾는 방법

코드
    Repeat {
              w := w -a dJ(w)/dw
    } // := 는 값을 갱신한다는 의미 / dJ(w)/dw는 J(w, b)가 w 방향으로 얼마나 기울었는지를 의미한다
    //알고리즘이 수렴할 때까지 반복한다
    
    J(w, b) -> w := w - a dJ(w, b)/dw
               b := w - a dJ(w, b)/db
**로지스틱회귀의 경사하강법을 위해 필요한 핵심 공식을 구현하는 방법** -> 계산그래프를 이용한다
z = w^t X + b
yhat = a = s(z)
L(a, y) = -(y log(a) +(1 - y) log(1 - a))


벡터화
---
벡터화 : for문을 없애주는 과정
될 수 있는 한 for문을 사용하지 않아야 한다 -> for문을 사용하는 것이 벡터화를 하는 것보다 300배 느리다
**로지스틱회귀의 벡터화**
<정방향 전파>
z^(1) = w^t X^(1) + b
a^(1) = s(z^(1))
z^(2) = w^t X^(2) + b
a^(2) = s(z^(2))
...(m회 수행) -> for문을 활용한다
**벡터화**
z = np.dot(w.T, x) + b

<비용함수>
for i = 1 to m
    z^(i) = w^T X^(i) + b
    a^(i) = s(z^(i))
J += -(y^(i) loga^(i) + (1-y) log(1-a^(i))
dz^(i) = a^(i) - y^(i)
dw1 += X^(i)1 dz^(i)
dw2 += X^(i)2 dz^(i)
db += dz^(i)
J /= m
dw1 /= m
dw2 /= m
db /= m
dw1 = dJ/dw1

w1 := w1 - a dw1
w2 := w2 - a dw2
b := b -a db
**벡터화**
Z = W^T X + b = np.dot(w.T, X) + b
A = s(Z)
dZ = A-Y
dw = 1/m dZ
db = 1/m np.sum(dZ)

*브로드캐스팅
파이썬 코드 실행 시간을 단축시켜준다
크기가 다른 행렬을 계산할 때 서로의 크기를 맞춰주는 것*

*NumPY코드
a = np.random.randn(5,1)
★행렬을 선언할 때 행 또는 열이 없도록 선언하지 않기 -> 이는 행벡터, 열벡터 모두 아니고, 계산 수행 시 이해할 수 없는 결과가 발생할 수 있다
=> 이 경우 a.reshape(n, m)을 통해 제대로 된 벡터로 변환해주어야 한다
a.shape(5, 1) //열벡터
a = np.random.randn(1, 5)
a.shape(1, 5) //행벡터
assert(a.shape == (5, 1)) //열벡터(5, 1)로 변환
*


신경망
---
  
신경망 : 입력값을 바탕으로 여러개의 은닉유닛을 가지고, 이를 종합하여 결과값을 알아낸다  
         은닉 유닛은 4개의 입려을 다 받는다. -> 입력값(X)와 서로 긴밀하게 연결되어 있다고 할 수 있다  
         충분한 양의 XY훈련예제를 준다면 X를 Y로 연결하는 함수가 뛰어나질 것이다.   
           
         신경망은 여러 활성화 함수 유닛을 쌓아서 만든것과 같다  
         입력값들이 여러 유닛에서 모두 사용되어 값을 도출하면 그 값으로 결과값 예측치를 찾아낸다.  
  
신경망을 통해 만들어진 경제적 가치들은 머신러닝의 한 종류인 지도학습에 의해 만들어졌다.  
**지도학습 : 입력 X와 출력 Y에 매핑되는 함수를 학습하려 한다**  
ex) 광고 정보와 유저 정보를 바탕으로 온라인 광고를 수행       ->      표준 신경망 구조  
    서로 다른 이미지들을 태깅                                ->      표준 신경망 구조  
    음성인식                                                 ->      CNN(합성곱 신경망)  
    기계번역                                                 ->      RNN(순환신경망)  
    자연어 처리                                              ->      RNN  
    앞 차에 대한 정보와 레이저 정보를 통한 자율주행           ->      더 복잡한 하이브리드  
  
데이터의 종류   
**구조적 데이터 : 데이터베이스로 표현된 데이터**  
                    ex) 주택가격(크기, 침실의 수 등), 광고 클릭(사용자 나이, 광고에 대한 정보 등)  
**비구조적 데이터 : 음성파일, 이미지, 텍스트 등 픽셀값이나 단어로 이루어진 데이터**  
-> 구조적 데이터보다 비구조적 데이터가 컴퓨터가 작업하기 훨씬 어렵다  
-> 딥러닝 덕분에 비구조적 데이터 분석이 많이 발전했다  
그러나 발생하는 경제적인 이익은 보통 구조적 데이터에서 많이 발생한다. 방대한 데이터를 기반으로 정확한 예측을 하는 경우 딥러닝이 유리하다  

**(i) : i번째 훈련 샘플 [l] : 신경망의 l번 레이어 a : 활성값  
입력 레이어 : 신경망의 입력층, 신경망의 층을 셀 때 입력레이어는 포함하지 않는다
은닉 레이어 : 훈련세트에서 무슨 값인지 알 수 없는 레이어로, 훈련세트에서 볼 수 없다고 해서 은닉 레이어라는 이름이 붙었다
출력 레이어 : 출력 값을 구하는 레이어**
로지스틱회귀에서는 z와 a를 구한 후 손실함수를 구하지만, 신경망에서는 z와 a를 반복해서 구하고 마지막에 손실함수를 계산한다
각 층에서 사용하는 활성화 함수는 모두 다를 수 있다.
  

  
딥러닝이 급부상한 이유  
---
  데이터의 양이 많아질수록 전통적인 학습 알고리즘의 성능은 정체기에 이른다  
  20년간 사회가 변화하면서 꽤 많은 데이터를 가지게 되었는데, 전통적인 학습 알고리즘이 효과적으로 처리할 수 있는 양 이상의 데이터가 쌓이게 되었다  
  많지 않은 데이터를 처리하는 부분에서 전통적인 학습 알고리즘은 딥러닝 학습 알고리즘보다 더 효과적일 수 있으나  
  데이터가 많아질수록 신경망이 큰 딥러닝 학습 알고리즘과 전통적인 학습 알고리즘의 성능 차이가 벌어지게 되었다  
    
**딥러닝의 아주 좋은 성능을 위해 필요한 두가지 요소**  
1. 많은 양의 데이터를 이용하기 위한 충분히 큰 신경망  
2. 많은 데이터  
신경망의 크기 = 많은 은닉유닛, 많은 연결, 많은 파라미터, 데이터의 규모   
성능을 올릴 때 가장 신뢰할 수 있는 방법 : 더 큰 신경 훈련망 또는 더 많은 데이터  
  
데이터의 규모가 딥러닝을 부상하게 만들었다  
  
딥러닝의 발전에는 방대한 양의 데이터, 컴퓨터의 빠른 계산능력, 그리고 알고리즘이 뒷받침한다  
초창기 딥러닝의 문제는 데이터와 계산의 규모에 있었다  
  기존의 시그모이드 함수는 양 끝단에서 미분값이 0과 가까워질수록 학습이 느려진다.  
  이를 ReLu함수를 이용하는 알고리즘으로 변화시키자 입력값이 양수인경우, 경사가 모두 1로 같고, 경사가 서서히 0으로 수렴할 확률이 줄어든다  
  즉, ReLu함수가 계산능력을 더 빠르게 만들었다  
  이는 빠른 계산능력에 영향을 미쳤다  
  빠른 계산이 중요한 이유는 많은 경우 신경망을 학습시키는 과정이 반복적이기 때문이다  
  **신경망을 학습시키는 과정 : 아이디어 -> 코드작성 -> 실험 -> 결과 -> 신경망 수정 -> 반복**  
  계산이 오래걸리면 위 사이클이 도는 시간도 오래걸리고, 결국 생산생에도 영향을 미친다  
  빠른 계산시간은 실험의 결과를 얻는 시간을 빠르게 만들었고, 이는 신경망을 사용하는 사람과 딥러닝 연구자들에게 아이디어를 더 빨리 연구할 수 있도록 도움을 주었다  
  
활성화함수
---
* 시그모이드함수  
        s(z) = 1/(1+e^-z)  
        주로 이진분류에서 사용  
        예측값은 0과 1사이의 값  
        평균값은 1/2  
        출력레이어에서 많이 사용한다  
        양 극단으로 갈수록 도함수가 0에 수렴한다 -> 경사하강법과 학습 속도가 느려진다  
                미분
                g'(z) = 1/(1+e^-z)
        
* tanh함수  
        tanh(z) = (e^z - e^-z)/(e^z + e^-z)  
        평균값이 0이기 때문에 대부분의 경우 시그모이드 함수보다 좋은 결과를 만들어낸다  
        양 극단으로 갈수록 도함수가 0에 수렴한다 -> 경사하강법과 학습 속도가 느려진다  
                미분
                g'(z) = 1 - (tanh(z))^2 = 1-a^2
                
* ReLU함수   
        a = max(0, z)  
        대부부의 경우 좋은 결과를 만들어낸다  
        대부분의 z에 대해 도함수가 0이 아니기 때문에 시그모이드함수와 tanh함수보다 유리하다  
        그러나 0미만의 값에서 도함수가 0이 된다. 대부분의 경우 z는 0보다 크기 때문에 문제는 없다  
        미분 불가능한 점인 z=0에서 기울기를 어떻게 보든 상관 없다
                미분
                g'z = 0 (z < 0)
                      1 (z >= 0)
        
* leakyReLU함수  
        a = max(0.01z, z)  
        0.01z도 알고리즘의 변수로 활용할 수 있다  
        ReLU함수가 0미만의 값에서 도함수가 0이되는 문제를 해결하기 위해 약간의 기울기를 부여했다  
                미분
                g'z = 0 (z < 0)
                      1 (z >= 0)
        
**주로 은닉 레이어에서는 ReLU를 제일 많이 사용하고, 가끔 tanh를 사용한다  
**출력 레이어에서는 주로 시그모이드를 사용하고, 실수인 경우 선형함수도 사용할 수 있다

**비선형 함수를 사용하는 이유**
선형함수를 사용하거나 활성화 함수를 사용하지 않는 경우 : 층이 얼마나 많든 신경망은 선형 함수만 계산한다
=> 그 결과 은닉층이 없는 것과 다름이 없다

변수 랜덤 초기화
---
모든 변수(파라미터)를 0으로 초기화하는 경우 **완전대칭문제**가 발생한다  
  
W^[1] = [[0 0][0 0]], b=[[0][0]]으로 초기화하는 경우  
a^[1]1 = a^[1]2  
n^[1]1과 n^[1]2 은닉 유닛은 모두 서로 같은 함수를 계산한다  
그 결과 두개의 활성이 같은 것이 되고, 가중치의 결과값도 항상 같은 결과를 만들어낸다(완전대칭문제)  
  
1층부터 두 유닛이 같은 함수를 계산하는 것으로 시작하여 출력 유닛에 항상 같은 영향을 미치게 되고, 첫번째 반복 이후에 항상 같은 상태가 계속해서 반복된다  
그 결과 두 은닉유닛은 실제로 한 개의 은닉유닛으로 볼 수 있다  
  
실은 파라미터 b는 0으로 초기화해도 문제가 없다  
  
이를 해결하기 위해 랜덤 초기화를 해야 한다  
**랜덤초기화 하는 방법**  
W^[1] = np.random.randn((2,2)) * 0.01  
      *0.01을 곱하는 이유 : 가중치의 초기값은 작을수록 좋다  
      왜냐하면 시그모이드 또는 tanh를 사용하면 양 극단의 경사 기울기가 작기 때문에 학습이 느려진다.  
      따라서 최대한 0과 가까운 값으로 초기화한다*  
b^[1] = np.zeros((2,1))  
W^[2] = np.random.randn((2,2)) * 0.01  
b^[2] = np.zeros((2,1))  

심층신경망
---
심층신경망 : 여러 신경망을 쌓은 것
             여러 은닉 레이어를 이용해(깊은 신경망) 원하는 결과값을 얻는다
             
input 사진 -> 특성 탐지
<간단한 함수 감지>       1. 모서리 탐지
 간단한 특징 감지        2. 사진에서 모서리가 어디있는지 알아본다 (수평 방향 모서리가 어디 있는지 등, 픽셀 그룹화)
                        3. 감지된 모서리와 그룹화된 모서리를 받아서 얼굴의 일부를 형성한다
                        4. 얼굴의 일부를 감지한다
<복잡한 함수 감지>       5. 얼굴의 일부를 모아 서로 다른 종류의 얼굴을 감지한다 (output)
 복잡한 특징 감지
 
 *심층신경망(딥러닝)이 필요한 이유
 X1 XOR X2 XOR X3 XOR ... ... XOR Xn을 계산할 때
 깊은 신경망을 이용하는 경우 트리형태를 만들면서 O(logn)으로 계산이 가능하다
 얕은 신경망을 이용하는 경우(1개의 은닉층) 은닉층에서 2^n가지의 가능한 모든 경우의 유닛을 만들어야 하고, n^[1]이 기하급수적으로 확장하게 된다. 결국 O(z^n)으로 계산하게 된다  
 로지스틱회귀 : 얕은 모델
 5개의 은닉 레이어 존재 : 깊은 모델*

**L : 네트워크의 수 n^[l] : l층에 있는 유닛의 개수 a^[l] : l층에서의 활성값 a^[0] : 입력특징(= X) a^[L] : 예측된 출력값(= y hat) w^[l] : z^[l]의 가중치 계산 b^[l] : z계산**

<정방향 전파 구현>
Z^[l] = W^[l] A^[l-1] + b^[l]
A^[l] = g^[l](Z^[l])

<역방향 전파 구현>
dZ^[l] = dA^[l] * g^[l](Z^[l])
dW^[l] = 1/m dZ^[l] A^[l-1]
db^[l] = 1/m np.sum(dZ^[l], axis = 1, keepdims = True)
dA^[l-1] = W^[l]T dZ^[l]

a^[0] ->   w^[1] b^[1]  -> a^[1] ->  w^[2] b^[2] -> a^[3] ->      ...      -> w^[l-1] b^[l-1] -> a^[l]
       z^[1] w^[1] b^[1]저장         z^[2] w^[2] b^[2] 저장                z^[l-1] w^[l-1] b^[l-1]저장      
       w^[1] b^[1] dz^[1] <- da^[1] <- w^[2] b^[2] dz^[2] <- da^[2] <- ... <- w^[l] b^[l] dz^[l] <- da^[l]
         dw^[1] db^[1]출력              dw^[2] db^[2] 출력                    dw^[l-1] db^[l-1] 

각 개별노드에서의 w, b, dw, db일반화
w^[l] : (n^[l], n^[l-1])
b^l : (n^[l], 1)
dw^[l] : (n^[l], n^[l-1])
db[l] : (n^[l], 1)

Z^[l] = g[l](a^[l]) : 둘의 차원이 동일해야 한다(n^[l], 1)

파라미터와 하이퍼파라미터
---
파라미터 : w, b
하이퍼파라미터 : a(학습률) : 매개변수가 어떻게 진전될지를 결정
                 수행하는 경사하강법의 반복 횟수
                 은닉층의 개수(L)
                 활성화 함수
                 모멘텀 항, 미니배치 크기, 다양한 형태의 정규화 매개변수 등
          => 학습 알고리즘에게 알려줘야 한다
하이터 파라미터를 통해 w, b를 통제한다


느낀점
---
행렬을 배워야 할 것 같다
