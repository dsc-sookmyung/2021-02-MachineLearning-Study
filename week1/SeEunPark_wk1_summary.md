# 딥러닝 소개

## 딥러닝이란?

신경망(Neural Network)을 학습시키는 것


## 신경망은 무엇인가?

- 신경망은 **입력(x)과 출력(y)을 매칭해주는 함수**를 찾는 과정
- 충분한 데이터가 주어지면 더 잘 알아낼 수 있음
- 해당 뉴런에 관계없는 입력값이라도 입력으로 넣어야 함.
관계 여부는 신경망이 학습하면서 알아서 조절해 감!
    
    <img width="500" alt="스크린샷_2021-09-28_오후_5 51 18" src="https://user-images.githubusercontent.com/66219968/135615101-c62293fe-d0b7-4d42-bb8b-42dba30fbe76.png">

예를 들어, 주택 가격 예측에서는 입력(x)는 주택에 관한 특성들, 출력으로는 가격을 예측하고자 하는 것!


## 신경망을 이용한 지도 학습

- 머신러닝의 방법은 지도 학습, 비지도 학습 등 여러 종류가 있음.
- **지도 학습**이란?
    - **정답이 주어져 있는 데이터를 사용**하여 컴퓨터를 학습시키는 방법
    - 신경망을 통해 만들어진 경제적 가치들은 지도학습을 통해 계산됨
    - 지도학습에서는 입력 X와 출력 Y에 매핑되는 함수를 학습하려고 함
    - 신경망을 통한 많은 값들의 생성은 어떤 문제를 해결하기 위한 적절한 X와 Y를 통해 이루어지고, 자율 주행같은 더 큰 시스템에 지도학습 요소들이 적합하게 함.

        <img width="500" alt="스크린샷_2021-09-28_오후_6 00 22" src="https://user-images.githubusercontent.com/66219968/135615280-a39920dc-6efa-49a0-8866-dd756b8c96f1.png">

- 조금씩 다른 신경망들은 **서로 다른 적절한 응용분야**에 적용됨.
    - 부동산 앱 : 표준 신경망 구조 사용
    - 부동산,온라인 광고 : 표준적인 신경망이 사용
    - 이미지 분야 : 주로 CNN 이라고 불리는 **합성곱 신경망** 사용
    - 음성같은 시퀀스 데이터 : 음성은 시간에 흐름에 따라 재생 → 1차원의 시계열 데이터로 나타나는 시퀀스 데이터, 주로 순환 신경망 (RNN) 사용
    - 영어, 중국어같은 언어도 알파벳이나 단어가 쭉 연결 → 시퀀스 데이터로, RNN의 조금 더 복잡한 버전들이 언어 분야에 사용

- **구조적 및 비구조적 데이터를 신경망을 사용하여 예측 가능**
    - 구조적 데이터
        - 데이터베이스로 표현된 데이터
            
            ⇒ 정보의 특성이 잘 정의
            
            - 주택 가격 예측 → 크기, 침실 개수 등
            - 사용자 광고 클릭 여부? → 나이, 광고에 대한 정보
            
    - 비구조적 데이터
        - 이미지, 오디오와 같이 특징적인 값 추출이 어려운 형태의 데이터
        - 딥러닝 덕분에 비구조적 데이터 인식 가능
        
    - 신경망에서 발생하는 경제적인 이익은 구조적 데이터에서 오는 경우가 많음. 정확한 예측을 만들어야하는 경우에 더 잘함.


## 딥러닝의 주요 성장 동력

- 지난 20년 동안 많은 양의 데이터를 보유하게 됨
    - 전통적인 학습 알고리즘이 효과적으로 처리할 수 있는 양 이상으로

        <img width="500" alt="스크린샷_2021-10-01_오후_7 12 02" src="https://user-images.githubusercontent.com/66219968/135615470-2481e19d-3f9a-4ee5-b847-4436a459604f.png">

⇒ 깊은 모델일 수록 더 많은 데이터 필요, 이는 곧 좋은 퍼포먼스.

 

- **딥러닝의 성능 향상 가능하게 도와준 3가지 요소**
    1. **데이터 양 증가**
    → 많은 데이터를 이용하기 위해 충분히 큰 신경망
    → 데이터의 규모가 딥러닝의 발전을 주도
        
        <img width="500" alt="스크린샷_2021-10-01_오후_7 25 50" src="https://user-images.githubusercontent.com/66219968/135615529-7deb6a12-f9ac-48d0-abd7-113f5a374419.png">
        
        - x축은 레이블이 있는 데이터를 의미 
        → 입력값 x와 레이블 y가 같이 있는 훈련세트(m)
        - 훈련할 데이터가 적으면 구현 방법에 따라 성능이 결정되는 경우 많음
        → 알고리즘의 상대적 순위의 정의가 잘 되어있지 않으므로, 특성을 다루는 실력이나 알고리즘의 작은 부분이 성능을 크게 좌우.
        - SVM을 훈련시키는데 여러 특성을 잘 관리한다면 구간 안에 있는 더 큰 신경망보다 SVM이 나을 수 있음.
        - 훈련세트가 아주 클 때, m이 아주 클 때만 큰 신경망이 일관되게 다른 방법을 압도하는 경향을 보임.
    2. **컴퓨터 성능 향상**
    3. **알고리즘의 개선**
    → Sigmoid 함수가 아닌 **ReLU 함수**를 사용함으로 Gradient 소멸 문제 해결
        
        <img width="500" alt="스크린샷_2021-10-01_오후_7 29 50" src="https://user-images.githubusercontent.com/66219968/135615640-0362723b-ba26-4648-b99a-c8a30130408e.png">
        
        - Sigmoid 함수
            - 함수의 경사가 0에 가까운 곳에서 학습이 굉장히 느림
            - 경사가 0일 때 경사하강법을 사용 → 파라미터가 아주 천천히 바뀌고 학습도 느려짐
        - 신경망의 활성화 함수 → ReUL함수로 바꾸면,
        입력값이 양수인 경우 경사가 1로 모두 같으므로 경사가 서서히 0에 수렴할 가능성이 훨씬 적음.
            
            ⇒ 알고리즘의 혁신으로 계산 능력 크게 향상시킴.
            
- 빠른 계산이 중요한 이유?
    
    <img width="500" alt="스크린샷_2021-10-01_오후_8 23 26" src="https://user-images.githubusercontent.com/66219968/135615654-f4845b2e-3148-4cac-a11d-e66bff54317c.png">
    
    - 아이디어(Idea) 생산 > 코드(Code) 구현 > 실험(Experiment)결과의 시간이 단축
    - 많은 경우 신경망을 학습시키는 과정이 반복적이기 때문 → 생산성 차이

# 신경망과 로지스틱회귀

## 이진 분류 (Binary Classification)

- 로지스틱 회귀 : 이진 분류를 위한 알고리즘

### 이진 분류에 대한 문제

- ex) 고양이 사진이 입력으로 주어졌을 때, 두 범주 중 어떻게 분류했는지 출력하길 원함
- 고양이 사진 → 1(cat), 0(non cat) 
⇒ y로 출력 레이블
- 사진이 어떻게 컴퓨터에 저장되는가?
    - Red, Green, Blue 세 개의 행렬을 따로 저장함.
    - 모든 픽셀값을 하나의 벡터로 펼치기 위해 주어진 사진에 대한 특성 벡터 x를 정의
    - 픽셀값 전부 픽셀 강도값으로 나타냄.
        - ex) 64*64 일 경우 벡터 x의 전체 차원은 64 * 64 * 3 = 12,288
        - n = n_x = 12,288
        

### 이진 분류의 목표

- 입력된 사진을 나타내는 특성 벡터 x를 가지고 그에 대한 레이블 y가 1, 0인지 예측할 수 있는 분류기를 학습하는 것!

### 표기법 소개

<img width="500" alt="스크린샷_2021-10-02_오전_12 29 56" src="https://user-images.githubusercontent.com/66219968/135751740-c665c821-4923-49f5-afb6-a9d53bc57eaa.png">

- 단 하나의 훈련 샘플을 한 쌍 (x, y)로 표시할 때, x는 n_x차원 상의 특성 벡터이고 레이블 y는 0 또는 1
- 훈련세트는 m개의 훈련샘플을 포함하고 (x^(1), y^(1)), (x^(2),y^(2)), (x^(3), y^(3)) ... (x^(m), y^(m))
    - 훈련 샘플의 개수를 m으로 표시, 강조하기 위해 m_train 라고 표기하기도 함.
    - 테스트 세트에 대해서는 테스트 세트의 개수를 m_test로 표시
- m이 훈련 샘플의 개수일 때 X는 m개의 열과 n_x개의 행들로 이루어짐.
    - X.shape = (n_x, m)
- 레이블 Y
    - 신경망의 구현을 더 쉽게하기 위해 y의 값들을 열로 놓는 것이 편리
    - Y = [y^(1), y^(2), ..., y^(m)] 로 이루어진 1xm 행렬
    - Y.shape = (1,m)

## 로지스틱 회귀

- 로지스틱 회귀란 답이 0 또는 1로 정해져있는 이진 분류 문제에 사용되는 알고리즘
- x (입력 특성), y (주어진 입력특성 x에 해당하는 실제 값), y^ (y의 예측값)을 의미
y^ 는 y가 1일 확률을 의미하며, 0≤ y^ ≤ 1 사이의 값을 가져야 함.
- x는 n_x 차원 상의 벡터, 로지스틱 회귀의 파라미터 w와 b는 각각 n_x차원 상의 벡터와 실수.
- 입력 x와 파라미터 w와 b가 주어졌을 때 y의 예측값을 출력하는 법
    - y^ = W^T X + b
        
        ⇒ 이진 분류를 위한 좋은 알고리즘은 아님
        ⇒  y의 예측값은 y가 1일 확률이기 때문에 항상 0과 1 사이어야하기 때문
        
    - 시그모이드 함수를 통해 0과 1 사이의 값으로 변환
        - z는 W^T X + b의 값을 의미
        - z가 실수 일 때,z의 시그모이드는 1/(1 + e^(-z))
            - 만약 z가 아주 크다면, e^(-z)가 0으로 수렴
                - z의 시그모이드는 1과 0에 가까운 숫자의 합을 1에서 나누었기 때문에 1에 가까움.
                - 즉, z가 아주 크면 z의 시그모이드가 1에 수렴
            - z가 아주 작거나, 아주 큰 음수일 경우, z의 시그모이드는 1/(1 + e^(-z))이고, e^(-z)가 아주 큰 수
                - 1과 엄청 큰 수를 1에서 나누었기 때문에 0과 가까워짐.
                - z가 큰 음수일 때, z의 시그모이드가 0에 수렴
        
        <img width="500" alt="스크린샷_2021-10-03_오후_5 21 32" src="https://user-images.githubusercontent.com/66219968/135751747-933857cc-3037-4f0c-a1ca-161db3f29fa7.png">
        
- 로지스틱 회귀를 구현할 때 y가 1일 확률을 잘 예측하도록 파라미터 w와 b를 학습해야함.

## 로지스틱 회귀의 비용함수

<img width="500" alt="스크린샷_2021-10-03_오후_5 35 55" src="https://user-images.githubusercontent.com/66219968/135751752-933160cf-8aa0-4528-af62-fff99d0e8107.png">

- 로지스틱 회귀 모델의 매개변수 w와 b를 학습하려면 비용함수를 정의해야함

### 손실 함수

- 하나의 입력특성(x)에 대한 실제값(y)과 예측값(*y*^) 의 오차를 계산하는 함수
    
    (= 단일 훈련 샘플이 얼마나 잘 작동하는지 측정)
    
- L(y^,y) = 1/2(y^-y)^2 식으로 사용하지만 로지스틱 회귀에서 이러한 손실 함수를 사용하면 지역 최소값에 빠질 수 있기 때문에 사용하지 않음.
- 로지스틱 회귀에서 사용하는 손실 함수
L(y^, y) = -(y * log y^) + (1-y) * log(1-y^)
    - 만약 제곱 오차를 사용하면, 그 오차를 최소화하려 할 것
    - y = 1 일 때, L(y^, y) = -log y^
        - -log y^ 값이 최대한 커지길 원함 → y^이 최대한 커야함.
        - but, y^은 시그모이드 함수값이기 때문에 1보다 클 수 없음.
        - 따라서, y^이 1보다 클 수 없으므로 1에 수렴하길 원함.
    - y = 0 일 때, 손실 함수는 -log(1-y^)
        - 손실 함숫값을 줄이기 위해선 -log(1-y^) 이 최대한 커야함. → y^이 최대한 작아야함.
        - y^는 시그모이드 함수값이기 때문에 0과 1 사이의 값이므로, y = 0 일 때, 손실 함수는 y의 예측값이 0에 수렴하도록 매개변수들을 조정
    - y가 1일 때, y의 예측값이 크고, y가 0일 때 y의 예측값이 작은 성질을 가지고 있는 함수들은 많음.
        - 위는 비공식적인 검증
    - 손실 함수는 훈련 샘플 하나에 관하여 정의돼서 하나가 얼마나 잘 예측 되었는지 측정해줌
    

### 비용함수

- 훈련 세트 전체에 대해 얼마나 잘 추측되었는지 측정해주는 함수
    
    (= 매개변수 w, b가 전체를 얼마나 잘 예측하는지 측정)
    
- J(w,b) = 손실 함수를 각각의 훈련 샘플에 적용한 값의 합들의 평균

<img width="500" alt="스크린샷_2021-10-03_오후_5 49 45" src="https://user-images.githubusercontent.com/66219968/135751755-1c459034-03b3-4a61-9a38-16780e5f1678.png">

- 비용 함수는 매개 변수의 비용처럼 작용한다는 점
- 로지스틱 회귀 모델을 학습하는 것은 손실 함수 J를 최소화해주는 매개 변수들 w와 b를 찾는 것

## 경사하강법

- 비용함수가 전체 데이터셋의 예측이 얼마나 잘 평가되었는지 보는 것이라면,
경사하강법은 이를 가능하게 한 w,b를 찾아내는 방법 중 하나!

<img width="500" alt="스크린샷_2021-10-03_오후_6 00 06" src="https://user-images.githubusercontent.com/66219968/135751757-4f032a6f-e27a-4b43-b727-5925190ab251.png">

- 매개변수 w,b를 알아내기 위해서는 비용함수를 가장 작게 만드는 w,b를 찾아야함.
- 경사 하강법 알고리즘을 이용하여 매개변수 w, b를 학습시키는 법
    - 볼록하지 않은 함수는 지역 최적값이 여러개! → 최적값을 찾을 수 없음.
    - 비용함수가 볼록하다는 사실이 로지스틱 회귀에 위의 비용함수 J를 사용한 가장 큰 이유!
    - 함수의 최소값을 모르기 때문에, 임의의 점을 골라서 시작 → 매개변수에 쓸 좋은 값을 찾기 위해 w,b를 초기화해야함
        - 주로 0으로 설정. 무작위는 잘 안 함.
            <img width="500" alt="스크린샷_2021-10-03_오후_6 03 05" src="https://user-images.githubusercontent.com/66219968/135751760-b271d935-43aa-4faa-a9dd-425a7eec5b8e.png">
        
    - 경사 하강법에서는 초기점에서 시작해 가장 가파른 내리막 방향으로 한 단계 내려감 → 전역 최적값이나 근사치에 도달하게 됨.

<img width="500" alt="스크린샷_2021-10-03_오후_6 12 53" src="https://user-images.githubusercontent.com/66219968/135751762-a9f8dd4c-512e-4285-998b-3223932893dd.png">

<img width="500" alt="스크린샷_2021-10-03_오후_6 23 45" src="https://user-images.githubusercontent.com/66219968/135751764-61689761-6b6f-460f-9061-5f857a6784f8.png">

- 만약 dw >0 이면, 파라미터 w 는 기존의 w 값 보다 작은 방향으로,
만약 dw <0 이면, 파라미터 w 는 기본의 w 값 보다 큰 방향으로 갱신됨.
- 도함수는 함수의 기울기
    - 도함수에 대한 표기
        
        <img width="500" alt="스크린샷_2021-10-03_오후_6 24 38" src="https://user-images.githubusercontent.com/66219968/135751766-67568ce0-81dc-47e9-a351-7d8b5644bc6a.png">
        
    

## 계산 그래프

- 신경망의계산은 정방향 패스 (정방향 전파는 긴경망의 출력값을 계산)
이는 역방향 패스 ,역방향 전파로 이어져 강사나 도함수를 걔산
- 예제) J(a, b, c) = 3(a  + bc) 일때, 함수 J의 계산은 다음의 순서를 따른다.
    
    <img width="500" alt="스크린샷_2021-10-03_오후_7 17 46" src="https://user-images.githubusercontent.com/66219968/135751769-201d4414-e8fb-437c-a12e-1b96343e43dd.png">
    
    1. u = bc
    2. v = a+ u
    3. J = 3v
- 미분의 연쇄법칙은 합성함수의 도함수에 대한 공식 → 합성함수를 구성하는 함수의 미분을 곱해서 구할 수 있음.
    - 입력변수 a 를 통해서 출력변수 J 까지 도달 하기 위해서 a→v→J 의 프로세스로 진행
    - 즉, 변수 a 만 보게 된다면, J = J(u(a)) 라는 함성함수가 될 것
    - 합성함수 a로 미분한 값 dJ/da를 구하기 위해 dJ/dv와 dv/da의 곱으로 표현
- 표기법
    - 최종 변수를 Final output var, 미분하려고 하는 변수를 var 라고 정의 한다면,
        
        **d Final output var / d var = d var**
        

## 로지스틱 회귀의 경사하강법

- 로지스틱 회귀의 경사하강법을 위해 필요한 핵심 공식 구현 방법
    - a는 로지스틱 회귀의 출력값, y는 참 값 레이블
        
        <img width="500" alt="스크린샷_2021-10-03_오후_8 00 01" src="https://user-images.githubusercontent.com/66219968/135751771-0dec1a4c-7c60-4629-9f87-08988a6a146e.png">
        
        <img width="500" alt="스크린샷_2021-10-03_오후_8 00 31" src="https://user-images.githubusercontent.com/66219968/135751774-2d58d7cc-8f33-4952-8e62-ab8a7bd9b415.png">
        
    - 손실함수의 도함수를 구하는 것이니 역방향으로 가서 에이에대한 손실함수의 도함수를 계산 → 코드에서는 da로 표시 가능
        
        <img width="500" alt="스크린샷_2021-10-03_오후_8 01 30" src="https://user-images.githubusercontent.com/66219968/135751775-06c4fdd8-603d-498d-b7bb-8c2c4614bf29.png">
        s
- m개의 훈련 샘플에 대해
    
    <img width="500" alt="스크린샷_2021-10-03_오후_8 15 55" src="https://user-images.githubusercontent.com/66219968/135751776-b6841ae9-33a6-4f29-bfcb-fa90e6a16905.png">



