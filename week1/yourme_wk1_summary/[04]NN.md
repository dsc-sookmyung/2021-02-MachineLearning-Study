# 4. 얕은 신경망 네트워크

Neural Network Representations
---
![2층 신경망](https://cphinf.pstatic.net/mooc/20180618_192/1529311676274xn6Qf_PNG/image.PNG)
- 입력층($a^{[0]}$) : 입력 특성($x_1,x_2,x_3$) 레이어
- 은닉층($a^{[l]}_n$) : 입력층과 출력층 사이에 있는 모든 층
	- 은닉층의 실제 값은 훈련 세트에 기록되어 있지 않음.
- 출력층: 예측 값인 $\hat{y}$ ($= a^{[2]}$) 계산


Computing Neural Network Output
---
- 입력값이 노드를 통과할 때 거치는 두 과정
	1. $z=w^Tx+b$
	2. $a = \sigma (z)$
- 벡터화를 사용하여 계산함
- $Given \ input \ x:$
	$z^{[1]}=W^{[1]}x+b^{[1]}$
		⇒ (4,1) = (4,3) (3,1) + (4,1)
	$a^{[1]}=\sigma (z^{[1]})$
		⇒ (4,1) = (4,1)
	$z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$
		⇒ (1,1) = (1,4) (4,1) + (1,1)　　; 이때, b는 실수 $\therefore$ $z^{[2]}$실수
	$a^{[2]} = \sigma (z^{[2]})$
		⇒  (1,1) = (1,1)
- $w=w^{[2]}, \ b=b^{[2]}$



Vectorizing Across Multiple Examples
---
- $a^{[i](j)}$
	- i : 몇 번째 층인지
	- j : 몇 번째 훈련 샘플인지
- m개의 모든 훈련 샘플 게산
	- for i = 1 to m:
			$z^{[1](i)}=W^{[1]}x^{(i)}+b^{[1]}$
			$a^{[1](i)}=\sigma (z^{[1](i)})$
			$z^{[2](i)}=W^{[2]}a^{[1](i)}+b^{[2]}$
			$a^{[2](i)} = \sigma (z^{[2](i)})$
	- X : 훈련 샘플이 열로 쌓은 행렬
	- **벡터화**
		$Z^{[1]}=w^{[1]}X+b^{[1]}$
		$A^{[1]}=\sigma (Z^{[1]})$
		$Z^{[2]}=w^{[2]}A^{[1]}+b^{[2]}$
		$A^{[2]}=\sigma (Z^{[2]})$
			⇒ $z^{[1](i)}, \ a^{[1](i)}, \ z^{[2](i)}, \ a^{[2](i)}$ 벡터들을 가로로 쌓아서 $Z^{[2]}, \ A^{[2]}$를 얻을 수 있음.
			⇒ 행렬 Z와 A의 가로는 **훈련 샘플의 번호**, 세로는 **신경망의 노드**
				- 행렬 A ⇒ 가로: 다른 훈련 샘플, 세로: 다른 은닉 유닛
				- 행렬 Z ⇒ 가로: 다른 훈련 샘플, 세로: 다른 입력 특성(신경망 입력층의 다른 노드들)


Activation functions
---
![enter image description here](https://cphinf.pstatic.net/mooc/20180622_130/1529646964215IJRni_PNG/plot0.png)

- Sigmoid 
	- $a = {1 \over 1 + e^{-z}}$
	- 이진 분류를 할 때 활성화 함수로 사용
	- 출력층에 시그모이드 활성화 함수 사용
- Tanh
	- $a = {e^{z}-e^{-z}} \over e^{z}+e^{-z}$
	- 대부분의 은닉층에서 tanh 활성화 함수 사용
	- 장점
		- 값이 [-1, 1] 사이에 있고 평균이 0 ⇒ 데이터를 원점으로 이동하는 효과
		- 평균이 0.5인 Sigmoid보다 더 효율적
		- 데이터의 중심을 0으로 해서 0.5인 시그모이드보다 더 쉽게 전달 가능
- ReLU
	- $a=max(0,z)$
	- 이진 분류를 제외란 대부분의 은닉층에서는 ReLU 사용
- leaky ReLU
	- $a=max(0.01z,z)$
	- 장점 : 0보다 큰 활성화 함수의 미분값이 다른 함수에 비해 많아서 빠르게 학습 가능

Why Non-linear Activation Functions
---
- 선형 활성화 함수 or 항등 활성화 함수 사용
	⇒ 신경망은 입력의 선형식 만을 출력하게 됨.
	if) 선형 활성화 함수 or 활성화 함수 X
	⇒ 신경망은 선형 활성화 함수만 계산 > 은닉층이 없는 것과 같음
- 선형 함수 g(z) = z 사용할 때 ⇒ 회귀 문제에 대한 머신러닝을 할 때(y가 실수일때 ex. 집값)
	

Derivatives Of Activation Functions
---
![enter image description here](https://cphinf.pstatic.net/mooc/20180622_108/1529646808652xnTuf_PNG/plot1.png)
![enter image description here](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https://blog.kakaocdn.net/dn/Ct5Yr/btqG6B6c6RB/32Sa5NXodL5FkLURB96du0/img.png)

Gradient descent for neural networks
---
단일층 신경망에서 경사 하강법을 구현하기 위한 방법
![enter image description here](https://user-images.githubusercontent.com/68985625/135485041-d64de4bf-f1b6-4a61-ae83-346d93a30f13.png)



Backpropagation Intuition
---
로지스틱 회귀의 역전파
- $da = -{y \over a} + {1-y \over 1-a}$
- $dz=a-y$
- $dw=dz \ x$
- $db = dz$
- $x$ : 고정값 ( $\therefore$ $dx$ 계산 X



Random Initialization
---
- 신경망을 훈련시킬 때 변수를 임의값으로 초기화하는 것은 중요함.
	- 로지스틱 회귀: 모두 0으로 초기화해도 괜찮음.
	- 신경망
		- $a^{[1]}_1$과 $a^{[1]}_2$가 같은 값을 가지게 됨. 
		⇒ 두 개의 활성이 같음( $\because$ 두 은닉 유닛 모두 정확히 같은 함수 계산)
		- $d^{[1]}_1$과 $d^{[1]}_2$이 같은 값을 가지게 됨.
			⇒ 대칭적인 결과를 가짐
			⇒ 두 은닉 유닛이 같은 값으로 초기화 되었기 때문에 **가중치의 결과값이 항상 같음**
			- 두 은닉 유닛이 같은 함수를 계산하는 것으로 시작
			⇒ 은닉 유닛이 출력 유닛에 항상 같은 영향을 줌
			⇒ 첫 번째 반복 이후에 같은 상태가 계속해서 반복됨
			⇒ 두 은닉 유닛은 항상 같은 함수를 가짐
			⇒ 쓸모없어짐 ( $\because$ 다른 함수 계산을 위한 각각 다른 유닛 필요)

- 신경망 초기화: 변수를 임의로 초기화하는 것(Random Initialization)
	- ``np.random.rand()`` 이용
		- ex. `` w_1 = np.random.rand((2,2)) * 0.01``
			- w가 너무 큰 값을 가지면 매우 큰 값의 z를 이용해 훈련을 시작 ⇒ tanh, sigmoid 활성 함수가 너무 큰 값을 가짐 ⇒ 학습 속도 느려짐
			- $\therefore$ 초기 변수들이 너무 큰 값을 가지지 않도록 해야함.
			- 이 외의 다른 작은 수를 곱해도 됨.
