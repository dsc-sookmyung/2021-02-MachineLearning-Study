# 2. 신경망과 로지스틱회귀

Binary Classification
---
+ 보통 신경망의 계산 과정
	- 정방형 패스(전파) 단계 뒤 역방향 패스(역전파)라는 단계가 있음
- 로지스틱 회귀: 이진 분류를 위한 알고리즘
+ 이진분류: 그렇다(1) / 아니다(0) 2개로 분류하는 것
	- 목표: 입력된 사진을 나타내는 특성벡터 x를 가지고 그에 대한 레이블 y가 1 아니면 0인지(고양이인지 아닌지) 예측할 수 있는 분류기를 학습하는 것

  

Logistic Regression
---
- x : 입력될 $n_x$차원 상의 특성 벡터,
- $\hat{y}=P(y=1 | x)$: y의 예측값
+ 파라미터
	- w : $n_x$차원 상의 벡터
	- b : 실수
- 입력 x와 파라미터 w와 b가 주어졌을 때 어떻게 y의 예측값을 출력?
- 잘 안 되는 방법: $\hat{y}=w^Tx+b$ ⇒ 선형회귀
$\because$ y의 예측값은 y가 1일 확률이기 때문에 항상 0과 1 사이어야함.
but, 위의 식은 1보다 넘거나 음수일 수 있음 ⇒ 확률값으로 정의 불가능
-  **로지스틱 회귀 : $\sigma(\hat{y}=w^Tx+b)$** ⇒ 시그모이드 함수를 적용한 값
- $\sigma(z) = {1\over1+e^z}$ ⇒ z가 커질수록 1에 가까워지고, z가 작을수록 0에 가까워짐
 

Logistic Regression Cost Function
---
- Given $\{(x^{(1)},y^{(1)}),...,(x^{(m)},y^{(m)})\}, want \ \hat{y}\approx y^{(i)}$
-  **매개변수 w와 b를 학습하기 위해 비용함수를 정의해야 함.**
-  **손실 함수** : 하나의 입력특성(x)에 대한 실제값(y)과 예측값( $\hat{y}$) 의 오차를 계산
	- 보통의 손실함수 : $L(\hat{y}-y) = {1  \over  2}(\hat{y}-y)^2$
but, 로지스틱 회귀에서 이러한 손실 함수를 사용하지 않음.
$\because$ 경사 하강법을 적용하지 못함
	-  **손실 함수**(in 로지스틱 회귀)
$L(\hat{y},y) = -(y\log\hat{y}+(1-y)\log(1-\hat{y}))$
		1.  **y = 1** ; $L(\hat{y},y) = -\log\hat{y}$ 가 0에 가까워지도록 $\hat{y}$는 1에 수렴
		2.  **y = 0** ; $L(\hat{y},y) = -\log(1-\hat{y})$가 0에 가까워지도록 $\hat{y}$는 0에 수렴
	-  **훈련 샘플 하나**에 관하여 정의되어서 그 하나가 얼마나 잘 예측되었는지 측정해줌.
-  **비용 함수** : 매개변수 w, b가 훈련 세트 전체를 얼마나 잘 예측하는지 측정해주는 함수
　　　　　모든 입력에 대한 오차를 계산
$J(w,b( = {-1\over m}{\sum_{i=1}^{i=m}}(y^{(i)}\log  \hat{y}^{(i)}+(1-(y^{(i)})\log(1-\hat{y}^{(i)}))$
⇒ 모든 입력에 대해 계산한 손실 함수의 평균 값
- 손실 함수 : 하나의 훈련 샘플에 적용
- 비용 함수 : 매개 변수의 비용처럼 작용
- 로지스틱 회귀 모델 학습 ⇒ 비용 함수 J를 최소화해주는 매개 변수들 w와 b를 찾는 것
- 비용 함수의 값이 작아지도록 하는 w와 b를 찾아야 함.

  

Gradient Descent
---
- 이전 시간의 비용함수가 전체 데이터셋의 예측이 얼마나 잘 평가되었는지 보는 것이라면, **경사하강법**은 이를 가능케하는 파라미터 w와 b를 찾아내는 방법 중 하나 입니다.
- 경사하강법:
	- $Want \ to \ find \ w,b \ that \ minimize \ J(w,b)$
	- 최적의 파라미터를 찾기 위해 비용 함수는 볼록한 형태여야 함
⇒ 볼록하지 않은 함수는 지역 최적값이 여러개임. (물결 형태)
	- 보통 초기값은 0 ⇒ 어느 곳에서 초기화 해도 같은 곳(가장 볼록한 곳)에서 만남
	- 함수의 최소값을 모르기 때문에, 임의의 점을 골라서 시작
	- 경사하강법은 가장 가파른(steepest) 방향,
즉 함수의 기울기를 따라서 최적의 값으로 한 스텝씩 업데이트
- 알고리즘
	- $w:=w-\alpha {dJ(w,b) \over dw}$
	- $b:=b-\alpha {dJ(w,b) \over db}$
	-  *α* : 학습률
⇒ 경사 하강법을 반복할 때 한 단계의 크기 결정
- ${dJ(w)\over dw} = dw$ : 미분계수(도함수)
⇒ 갱신할 때 매개변수 $w$에 줄 변화를 나타냄
- $:=$ 갱신
- $dw > 0$ ; 파라미터 w 는 기존의 w 값 보다 작은 방향으로 업데이트
$dw < 0$ ; 파라미터 w 는 기본의 w 값 보다 큰 방향으로 업데이트
- 도함수는 함수의 기울기라고 볼 수 있습니다. 다음 시간에 조금 더 자세히 설명하겠습니다.
- 하나의 변수에 대한 도함수는 $dw = {df(w)\over dw}$ 라고 표기하지만 두 개 이상은 보통 아래와 같이 표현 합니다.
- $dw = {\partial J(w,b) \over  \partial w}$ : 함수의 기울기가 w 방향으로 얼만큼 변했는지 나타냅니다.
- $*db = {\partial J(w,b) \over  \partial b}*$ : 함수의 기울기가 b 방향으로 얼만큼 변했는지 나타냅니다.
  


Computation Graph
---
- 신경망의 계산
정방향 패스(전파) : 신경망의 출력값 계산
⇒ 역방향 패스(전파)로 이어짐 : 경사, 도함수 계산

- 계산 그래프: 특정한 출력값 변수(J)를 최적화하고 싶을 때 유용
- 왼쪽에서 오른쪽으로 계산하는 파란색 화살표 사용
- 예제: $J(a,b,c) = 3(a+bc)$
	1. $u = bc$
	2. $v = a+u$
	3. $J = 3v$

  

Derivatives With Computation Graphs
---
  - 미분의 연쇄법칙 : 합성함수의 도함수에 대한 공식
합성함수의 도함수는 합성함수를 구성하는 함수의 미분을 곱함으로써 구할 수 있습니다.
	- 입력변수 a 를 통해서 출력변수 J 까지 도달 하기 위해서 $*a→v→J*$ 의 프로세스로 진행됩니다. 즉, 변수 a 만 보게 된다면, J = J(u(a)) 라는 함성함수가 될것 입니다.
	- 합성함수를 a로 미분한 값 ${dJ \over da} = {dJ \over dv} \times {dv \over da}$
- 표기법 정의
	- 최종변수(최적화 하려는 값 = J)를 Final output var, 미분하려고 하는 변수를 var 라고 정의 한다면
	- ${d \ Final \ ouput \ var \over d \ var} = d \ var$

  

Logistic Regression Gradient Descent
---
로지스틱 회귀의 경사하강법을 위해 필요한 핵심 공식을 구현하는 방법(with 계산 그래프)
- $z=w^T+b$
$\hat{y}=a=\sigma(z)$　　　($\hat{y}$: 예측값)
$L(a,y)=-(y\log(a)+(1-y)\log(1-a))$
　　　　($L$: 손실함수, $a$: 로지스틱 회귀의 출력값, $y$: 참 값 레이블)
- $z=w_1x_1+w_2x_2+b$ ⇒ $\hat{y}=a=\sigma(z)$ ⇒ $L(a,y)$
	- 로지스틱 회귀 목적: 매개변수 w와 b를 변경해서 L 손실을 줄이는 것.
	- 도함수의 계산
		* a에 대한 손실함수의 도함수
$da={dL(a,y) \over da}=-{y\over a}+{1-y \over  1-a}$
		* z에 대한 손실함수의 도함수
$dz={dL \over dz} = a - y$
		* $w_1$에 대한 손실함수의 도함수
$dw_1={dL \over dw_1} = x_1dz$
		* b에 대한 손실함수의 도함수
$db={dL\over db}=dz$

  

Gradient Descent on m Examples
---
- 비용함수 : $J(w,b) = {1\over m}{\sum_{i=1}^{i=m}}(L(a^{(i)},y^{(i)}))$
- 코드 표현
- 특성의 개수가 많아진다면 for문을 이용해 처리해야함.
> 이중 for문으로 인해 계산 속도 저하
- 단일 샘플의 도함수 평균 ⇒ 경사 하강법에 사용할 전체 경사값
