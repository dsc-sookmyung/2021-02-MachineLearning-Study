# Regularizing your neural network

## Regularization

- 높은 분산으로 신경망이 데이터를 과대적합하는 문제가 의심된다면,
    - 가장 먼저 정규화 시도 → 과대적합을 막고 신경망의 분산을 줄이는데 도움
    - 더 많은 훈련 데이터를 얻는 것 → 비용이 많이 들어감
- L1보다는 L2 정규화를 보통 많이 사용
- 정규화는 훈련 속도를 빠르게 하고 모델에 어떠한 해도 가하지 않기 때문에 웬만하면 하는 게 좋음.
    
    <img width="500" alt="스크린샷_2021-10-10_오후_8 13 38" src="https://user-images.githubusercontent.com/66219968/136709764-c6b26d99-f5d0-4d7e-baed-2d2164f3fc31.png">
    

## Why Regularization Reduces Overfitting

- λ 값을 크게 만들어서 가중치행렬 w 를 0에 가깝게 설정할 수 있음.
    - 많은 은닉 유닛을 0에 가까운 값으로 설정 → 더 간단하고 작은 신경망
- tanh 활성화 함수를 사용할 경우 λ 값이 커지면,
비용함수에 의해 w는 작아지게 되고, 이때 z^[l] = w^[l]a^[l-1]+b^[l]이므로 z도 작아지게 됨.
    - z 가 작을 때 g(z)는 선형 함수가 되고, 전체 네트워크도 선형이 되기에
    과대적합과 같이 복잡한 결정을 내릴 수 없음.
    
    <img width="500" alt="스크린샷_2021-10-10_오후_8 22 15" src="https://user-images.githubusercontent.com/66219968/136709768-1f8a9966-7dbe-4afa-84f6-0856d42c1479.png">
    

## Dropout Regularization

- 드롭아웃  : 신경망의 각각의 층에 대해 노드를 삭제하는 확률을 설정하는 것
    - 삭제할 노드를 랜덤으로 선정 후 삭제된 노드의 들어가는 링크와 나가는 링크를 모두 삭제
    - 그럼 더 작고 간소화된 네트워크가 만들어지고 이때 이 작아진 네트워크로 훈련을 진행
- 역 드롭아웃 : 노드를 삭제후에 얻은 활성화 값에 keep.prop을 나눠 주는 것
    - 이는 기존에 삭제하지 않았을 때 활성화 값의 기대값으로 맞춰주기 위함

## Understanding Dropout

- 드롭아웃은 무작위로 신경망의 유닛을 삭제 → 하나의 특성에 의존하지 못 하게 함 → 가중치를 다른 곳으로 분산 → 가중치의 노름의 제곱값이 줄어들게 됨.
- 드롭아웃의 keep.prop 확률은 층마다 다르게 설정할 수 있음.
- 모든 반복에서 잘 정의된 비용함수가 하강하는지 확인하는게 어려워짐.
→ 우선 드롭아웃을 사용하지 않고, 비용함수가 단조감소인지 확인 후에 사용

## Other Regularization Methods

- L2 정규화와 드롭아웃 정규화와 더불어 신경망의 과대적합을 줄이는 다른 기법들
    - **Data Augmentation (데이터 증식)**
        - 이미지 → 더 많은 훈련 데이터 사용 ex) 대칭, 확대, 왜곡, 회전
            - 독립적인 샘플을 얻는 것보다 많은 정보를 추가해주진 않지만, 컴퓨터적인 비용이 들지 않고 할 수 있음.
        - 시각적인 문자의 인식 → 숫자를 얻어 무작위의 회전과 왜곡 부여
    - **Early Stopping (조기 종료)**
        - 훈련 오차나 비용함수 J는 단조 하강하는 형태
        - 개발 세트의 오차도 그려줌
        - 개발 세트 오차가 어느 순간부터 하락하지 않고 증가하기 시작하는 것 → 과대적화되는 시점
            
            ⇒ 조기 종료는 신경망이 개발 세트의 오차 저점 부근, 가장 잘 작동하는 점을 때 훈련을 멈춤.
            
        - But,  조기 종료는비용 함수를 최적화 시키는 작업과 과대적합하지 않게 만드는 작업을 섞기 때문에 최적의 조건을 찾지 못 할 수 있음.
