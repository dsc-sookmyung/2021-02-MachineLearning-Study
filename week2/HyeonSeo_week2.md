# Week2 ML study
신경망 - 층, 은닉유닛 수, 학습률, 활성화함수 등을 정해야 하는데 곧바로 가장 적절한 값을 찾을 수 없기 때문에 반복적인 과정을 거쳐야 한다
아이디어 -> 특정개수의 층과 유닛, 특정 데이터 세트에 맞는 신경망을 만든다 -> 코딩, 실행, 실험 등 진행 -> 아이디어 개선 (반복)

## 신경망 세트
trainning set : 훈련 알고리즘 제작
dev set : 교차검증 개발세트, 서로 다른 알고리즘 중 가장 좋은 성능을 내는 것을 찾는다
test set : 최종 모델 알고리즘이 얼마나 잘 작동하는지 test
과거 : trainning set 70% | test set 30% 또는
       trainning set 60% | dev set 20% | test set 20%
       => 데이터의 수가 많아짐에 따라 dev와 test 셋의 수가 20%나 차지할 필요가 없어짐
현재 : trainning set 98% | dev set 1% | test set 1% 또는
       trainning set 99.5% | dev set 0.5% | test set 0.5%
       
       최종 네트워크의 비편향 추정이 필요한 경우에만 테스트 세트가 필요
       -> 테스트 세트를 사용하지 않는 경우 존재
          테스트 세트라고 하지만 실제로는 교차검증을 위해 사용하는 경우 (test세트가 아니라 dev세트인 경우)
       
## 편향과 분산
train set Error와 dev set Error을 비교함으로 편향문제와 분산문제를 파악할 수 있다
##### train set error : 훈련데이터에서 알고리즘이 얼마나 적합한지 감을 잡을 수 있다, 편향문제
##### train set error와 dev set error와의 차이 : 분산문제가 얼마나 나쁜지 감을 잡을 수 있다, 일반화를 잘 하느냐에 따라 분산에 대한 감이 달라진다
높은 편향값 : 데이터 과소적합
높은 분산 : 데이터 과대적합
ex) 인간의 오차(최적 오차, 베이즈 오차) ~ 0인경우
    train set error : 1% dev set error : 11% => 데이터 과대적합, 일반화가 제대로 되지 않음 -> 높은 분산을 가짐
    train set error : 15% dev set error : 16% => 상대적으로 훈련데이터에 잘 맞지 않지만(데이터 과소적합) train set error와 dev set error 간 차이가 1%밖에 되지 않기 때문에 합리적 수준의 개발세트로 사용된다
    train set error : 15% dev set error : 30% => 데이터 과소적합, 일반화가 제대로 되지 않음 -> 높은 편향과 분산을 가짐
    train set error : 0.5% dev set error : 1% => 낮은 편향과 낮은 분산을 가짐
    
    인간의 오차(베이즈오차) ~ 15%인경우
    train set error : 15% dev set error : 16% => 낮은 편향과 분산을 가진다
    ex)흐릿한 이미지

## 머신러닝 성능을 쳬계적으로 향상시키는 
1차 모델 훈련 -> 높은 편향을 가지는지 확인 : 훈련 set 데이터를 확인
       경우1. 높은 편향을 가지는 경우 : 더 많은 은닉층 사용 or 더 많은 은닉유닛 사용 or 더 오래 훈련 or 다른 최적화 알고리즘 사용 (or 다른 신경망 아키텍처 사용 - 작동할수도, 안할수도 있음) -> 모델 훈련부터 다시 시작(반복)
       경우2. 수용가능한 편향을 가지는 경우 -> 분산문제가 있는지 확인
              경우 2-1. 높은 분산을 가지는 경우 : 더 많은 데이터 이용 or 정규화 (or 다른 신경망 아키텍저 - 작동할수도, 안할수도 있음) -> 모델 훈련부터 다시 시작(반복)
              경우 2-2. 적당한 분산을 가지는 경우 -> 끝

## 편향-분산 트레이드 오프
과거(딥러닝 이전) : 시도할 수 있는 많은 툴이 없었다
                    편향이 오르고 분산이 내리거나
                    편향이 내리고 분산이 오르거나
현재(딥러닝 시대) : 많은 데이터를 가지고도 편향은 유의미한 변화가 없는데 분산이 내려가는 등 편향과 분산이 서로 영향을 주지 않고 작아지는 방법이 존재하다
                    -> 딥러닝이 유용하다 (가끔 어쩔 수 없이 편향이 오르고 분산이 내려가거나, 편향이 내리고 분산이 올라가는 경우가 존재)

## 정규화
높은 분산(데이터 과대적합)인 경우 분산을 낮춰주기 위한 방법
lambd : 정규화 매개변수 : 두매개변수의노름을 잘 설정해 과대적합을 막을 수 있는 최적의 값 탐색
        설정이 필요한 하이퍼 파라미터
        lambda는 파이썬 함수의 이름이기 때문에 이와 혼동하지 않기 위해 lambd로 표기
로지스틱회귀에서 L2정규화 : J(w, b) = 1/m * L(y_hat^(i), y^(i)) + lambd/2m * ||w||^2(sub)2 + (lambd/2m * b^2)-보통생략
w : x차원 벡터
b : 실수
||w||^2(sub)2 : 유클리드 노름의 제곱
w : 꽤 높은 차원의 매개변수 벡터, 분산이 높을수록 많은 매개변수를 포함
b : 하나의 실수 -> 넣어도 실질적인 변화가 없음 => 보통 생략한다

L1정규화 : lambd/2m * ||w||^2(sub)2  대신 lambd/m * S(i=1부터 n_x까지)|w| = lambd/m ||w||1 
w는 희소하다(w안에 0이 많다) -> 모델 압축에 도움이 되지만 실질적으로 큰 도움이 되지 않는다
=> L2에 비해 잘 사용하지 않는다

신경망에서 L2 정규화 : J(w^[1], b^[1],...,w^[L], b^[L]) = 1/m S(i=1부터 n까지)L(y_hat^(i), y^(i)) + 1/2m S(l=1부터 L까지)||w^[l]||^2(sub)F
||w^[l]||^2(sub)F : 프로베니우스 노름 : 행렬 원소 제곱의 합
w : (n^[l-1], n^[l]) : l-1과 l의 은닉유닛 개수
-> 경사하강법
dw역전파 : dJ/dw^[l] + lambd/m * w^[l]
w^[l] := w^[l] - adw^[l] 
       = w^[l]a[(역전파로 얻은 것) + lambd/m * w^[l]]
       = w^[l] - alambd/m w^[l] -a(역전파로 얻은 것)
       = w^[l](1-alambd/m)
       = w^[l] - alambd/m * w^[l]
       => w^[l]이 어떤 값이든 값이 약간 더 작아진다 => L2정규화 : 가중치 감쇠
       
### 정규화를 하면 왜 과대적합 문제를 줄일 수 있는가?
프로베니우스 노름이 작아질수록 과대적합 문제가 줄어든다

1. 비용함수 J(w^[l],b^[l]) = 1/m S(i=1부터 n_x까지)L(y_hat^(i), y^(i)) + lambd/2m S(l=1부터 L까지)||w^[l]||^2_F
       lambd를 크게 하면 w^[l]이 0에 가까워진다
       은닉유닛을 0에 가깝게 하면 은닉유닛의 영향이 줄어들고, 그 결과 이전보다 간단하고 작은 신경망이 된다
       간단한 네트워크는 과대적합이 덜 나타나게 된다
2. tanh함수를 활성화 함수로 쓰는 경우 lambd가 커질 때 비용함수가 커지지 않으려면 w^[l]가 작아진다
       z^[l] = w^[l]a^[l-1] + b^[l]
       w^[l]이 작아지면 z^[l]도 작아지게 된다
       z^[l]이 작아지면 tanh함수로 인해 g(z)는 거의 선형함수가 된다
       모든 레이어가 거의 선형이 되면 전체 네트워크도 거의 선형이 된다

*경사하강법을 구현할 때 비용함수를 경사하강법의 반복 수에 대한 함수로 두면 반복을 진행할 때마다 단조감소하는 함수로 표현할 수 있다.
이를 위해서는 프로베이누스 노름의 합 항까지 모두 포함하여 구현해주어야 한다

## 드롭아웃 정규화
신경망의 각각의 층에 대해 노드를 삭제할 확률을 추가한다
노드 삭제 -> 작고 간소한 네트워크

정규화의 한 방법으로, 네트워크의 과대적합 문제가 발생하기 전까지 드롭아웃을 사용하지 않는다

단점 : J(비용함수)가 더이상 잘 정의되지 않아 비용함수가 잘 하강하는지 확인하기 어렵고, 디버깅도 어렵다
       => 드롭아웃 적용 이후에 코드를 수정하지 않는다

**역드롭아웃**
l = 3,  keep_prob = 0.8
*d3 = np.random.rand(a3.shape[0], a3.shape[1] < keep_prob  #무작위로 행렬 생성
a3.np.multiply(a3, d3)

a3 /= keep_prob
       20%의 확률로 노드 삭제 -> a3가 원래의 20%만큼 감소한다
       이를 해결하기 위해 필요한 20%만큼 채워주어야 하는데 keep_prob로 나누면 그만큼을 채워준다
       이를 통해 스케일링 문제를 해결하고, 테스트 단계에서 드롭아웃을 하지 않아도 활성화 기댓값의 크기가 변화하지 않는다*

test단계에서는 드롭아웃을 사용하지 않는다(노드를 삭제하지 않는다)
       결과가 무작위로 나오기를 바라지 않기 때문

드롭아웃을 하면 입력을 무작위로 삭제하기 때문에 어떤 특성에도 의존할 수 없다. (입력 값이 무작위로 바뀔 수 있기 때문)
=> 특정 입력에 큰 가중치를 주지 않는다
=> 각각 입력에 가중치를 분산시킨다
=> 가중치 노름의 제곱값 감소
=> 과대적합을 막는다(L2정규화와 비슷한 효과가 발생한다)

각 층마다 가중치를 다르게 적용할 수 있다

## 그 밖의 정규화방법
### 훈련세트를 늘린다
훈련세트를 추가적으로 얻을 수 없는 경우인 경우, 혹은 적은 비용으로 추가적인 데이터를 얻기 위해
**사진을 수평방향으로 반전시키거나 무작위로 편집해 새로운 샘플을 만든다**
중복되는 샘플이 많아져 좋지 않지만, 훈련세트가 늘어나는 효과가 있다

**조기종료**
경사하강법을 하게 되면, 훈련오차를 그리거나 최적화하는 비용함수 J를 그리게 된다
이와 함께 개발오차도 그린다
-> 훈련오차와 개발세트오차 사이에서 신경망이 가장 잘 작동하는 순간을 고른다
신경망을 적게 반복할수록 w는 0에 가깝고, 오랜시간동안 훈련하면서 w의 값이 커진다
=> w에 대해 더 작은 노름을 갖는 신경망을 선택하게 된다
       장점 : 경사하강법 과정을 한 번 시도함으로 큰w, 중간w, 작은w를 구할 수 있다
       단점 : ML과정은 여러 과정으로 구성되어 있는데, 그 중 1. 비용함수 최적화 알고리즘 -> 2. 과대적합 문제 해결 의 과정에서 조기종료는 1과 2의 작업이 섞여버린다
              결국 J의 최적화 값을 잘 구하지 못하면서 과대적합을 막게 되어 문제를 더 복잡하게 만들게 된다
              => 이를 해결하기 위해 L2정규화를 사용할 수 있다

## 다중클래스 분류 softmax
softmax : 신경망의 최종 층에서 z^[L]을 게산한다
클래스 C가 여러 개인 경우 사용
특이한 점 : 입, 출력값이 모두 벡터, 클래스 사이의 관계가 선형

Z^[L] = [[5],[2],[-1],[3]]인 경우 이의 활성화 함수를 사용하기 전에 새로운 변수 t를 이용
t  = [[e^5],[e^2],[e^-1],[e^3]]
=> a^[L] = e^z^[L] / S(j=1부터 4까지) ti   -> (4, 1)벡터
a^[t]i = ti/S(j=1부터 4까지)ti
g^[L](z^[L]) = [[e^5/e^5+e^2+e^-1+e^3],[e^2/e^5+e^2+e^-1+e^3],[e^-1/e^5+e^2+e^-1+e^3],[e^3/e^5+e^2+e^-1+e^3]]

C=2인경우 로지스틱 회귀를 이용하는 것과 같다

신경망 학습 시 손실함수
L(y_hat, y) = -S(j=1부터 4까지)yi * log(y_hatj)
위를 최소화해야 함, y_hat의 확률 최대화
비용함수
J(w^[1], b^[1], ...) = 1/mS(i=1부터 m까지)L(yhat^(i), y(i))
경사하강법을 통해 
