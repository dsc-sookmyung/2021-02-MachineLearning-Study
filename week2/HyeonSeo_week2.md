# Week2 ML study
신경망 - 층, 은닉유닛 수, 학습률, 활성화함수 등을 정해야 하는데 곧바로 가장 적절한 값을 찾을 수 없기 때문에 반복적인 과정을 거쳐야 한다
아이디어 -> 특정개수의 층과 유닛, 특정 데이터 세트에 맞는 신경망을 만든다 -> 코딩, 실행, 실험 등 진행 -> 아이디어 개선 (반복)

## 신경망 세트
trainning set : 훈련 알고리즘 제작
dev set : 교차검증 개발세트, 서로 다른 알고리즘 중 가장 좋은 성능을 내는 것을 찾는다
test set : 최종 모델 알고리즘이 얼마나 잘 작동하는지 test
과거 : trainning set 70% | test set 30% 또는
       trainning set 60% | dev set 20% | test set 20%
       => 데이터의 수가 많아짐에 따라 dev와 test 셋의 수가 20%나 차지할 필요가 없어짐
현재 : trainning set 98% | dev set 1% | test set 1% 또는
       trainning set 99.5% | dev set 0.5% | test set 0.5%
       
       최종 네트워크의 비편향 추정이 필요한 경우에만 테스트 세트가 필요
       -> 테스트 세트를 사용하지 않는 경우 존재
          테스트 세트라고 하지만 실제로는 교차검증을 위해 사용하는 경우 (test세트가 아니라 dev세트인 경우)
       
## 편향과 분산
train set Error와 dev set Error을 비교함으로 편향문제와 분산문제를 파악할 수 있다
##### train set error : 훈련데이터에서 알고리즘이 얼마나 적합한지 감을 잡을 수 있다, 편향문제
##### train set error와 dev set error와의 차이 : 분산문제가 얼마나 나쁜지 감을 잡을 수 있다, 일반화를 잘 하느냐에 따라 분산에 대한 감이 달라진다
높은 편향값 : 데이터 과소적합
높은 분산 : 데이터 과대적합
ex) 인간의 오차(최적 오차, 베이즈 오차) ~ 0인경우
    train set error : 1% dev set error : 11% => 데이터 과대적합, 일반화가 제대로 되지 않음 -> 높은 분산을 가짐
    train set error : 15% dev set error : 16% => 상대적으로 훈련데이터에 잘 맞지 않지만(데이터 과소적합) train set error와 dev set error 간 차이가 1%밖에 되지 않기 때문에 합리적 수준의 개발세트로 사용된다
    train set error : 15% dev set error : 30% => 데이터 과소적합, 일반화가 제대로 되지 않음 -> 높은 편향과 분산을 가짐
    train set error : 0.5% dev set error : 1% => 낮은 편향과 낮은 분산을 가짐
    
    인간의 오차(베이즈오차) ~ 15%인경우
    train set error : 15% dev set error : 16% => 낮은 편향과 분산을 가진다
    ex)흐릿한 이미지

## 머신러닝 성능을 쳬계적으로 향상시키는 
1차 모델 훈련 -> 높은 편향을 가지는지 확인 : 훈련 set 데이터를 확인
       경우1. 높은 편향을 가지는 경우 : 더 많은 은닉층 사용 or 더 많은 은닉유닛 사용 or 더 오래 훈련 or 다른 최적화 알고리즘 사용 (or 다른 신경망 아키텍처 사용 - 작동할수도, 안할수도 있음) -> 모델 훈련부터 다시 시작(반복)
       경우2. 수용가능한 편향을 가지는 경우 -> 분산문제가 있는지 확인
              경우 2-1. 높은 분산을 가지는 경우 : 더 많은 데이터 이용 or 정규화 (or 다른 신경망 아키텍저 - 작동할수도, 안할수도 있음) -> 모델 훈련부터 다시 시작(반복)
              경우 2-2. 적당한 분산을 가지는 경우 -> 끝

## 편향-분산 트레이드 오프
과거(딥러닝 이전) : 시도할 수 있는 많은 툴이 없었다
                    편향이 오르고 분산이 내리거나
                    편향이 내리고 분산이 오르거나
현재(딥러닝 시대) : 많은 데이터를 가지고도 편향은 유의미한 변화가 없는데 분산이 내려가는 등 편향과 분산이 서로 영향을 주지 않고 작아지는 방법이 존재하다
                    -> 딥러닝이 유용하다 (가끔 어쩔 수 없이 편향이 오르고 분산이 내려가거나, 편향이 내리고 분산이 올라가는 경우가 존재)

## 정규화
높은 분산(데이터 과대적합)인 경우 분산을 낮춰주기 위한 방법
lambd : 정규화 매개변수 : 두매개변수의노름을 잘 설정해 과대적합을 막을 수 있는 최적의 값 탐색
        설정이 필요한 하이퍼 파라미터
        lambda는 파이썬 함수의 이름이기 때문에 이와 혼동하지 않기 위해 lambd로 표기
로지스틱회귀에서 L2정규화 : J(w, b) = 1/m * L(y_hat^(i), y^(i)) + lambd/2m * ||w||^2(sub)2 + (lambd/2m * b^2)-보통생략
w : x차원 벡터
b : 실수
||w||^2(sub)2 : 유클리드 노름의 제곱
w : 꽤 높은 차원의 매개변수 벡터, 분산이 높을수록 많은 매개변수를 포함
b : 하나의 실수 -> 넣어도 실질적인 변화가 없음 => 보통 생략한다

L1정규화 : lambd/2m * ||w||^2(sub)2  대신 lambd/m * S(i=1부터 n_x까지)|w| = lambd/m ||w||1 
w는 희소하다(w안에 0이 많다) -> 모델 압축에 도움이 되지만 실질적으로 큰 도움이 되지 않는다
=> L2에 비해 잘 사용하지 않는다

신경망에서 L2 정규화 : J(w^[1], b^[1],...,w^[L], b^[L]) = 1/m S(i=1부터 n까지)L(y_hat^(i), y^(i)) + 1/2m S(l=1부터 L까지)||w^[l]||^2(sub)F
||w^[l]||^2(sub)F : 프로베니우스 노름 : 행렬 원소 제곱의 합
w : (n^[l-1], n^[l]) : l-1과 l의 은닉유닛 개수
-> 경사하강법
dw역전파 : dJ/dw^[l] + lambd/m * w^[l]
w^[l] := w^[l] - adw^[l] 
       = w^[l]a[(역전파로 얻은 것) + lambd/m * w^[l]]
       = w^[l] - alambd/m w^[l] -a(역전파로 얻은 것)
       = w^[l](1-alambd/m)
       = w^[l] - alambd/m * w^[l]
       => w^[l]이 어떤 값이든 값이 약간 더 작아진다 => L2정규화 : 가중치 감쇠
       
### 정규화를 하면 왜 과대적합 문제를 줄일 수 있는가?
프로베니우스 노름이 작아질수록 과대적합 문제가 줄어든다

1. 비용함수 J(w^[l],b^[l]) = 1/m S(i=1부터 n_x까지)L(y_hat^(i), y^(i)) + lambd/2m S(l=1부터 L까지)||w^[l]||^2_F
       lambd를 크게 하면 w^[l]이 0에 가까워진다
       은닉유닛을 0에 가깝게 하면 은닉유닛의 영향이 줄어들고, 그 결과 이전보다 간단하고 작은 신경망이 된다
       간단한 네트워크는 과대적합이 덜 나타나게 된다
2. tanh함수를 활성화 함수로 쓰는 경우 lambd가 커질 때 비용함수가 커지지 않으려면 w^[l]가 작아진다
       z^[l] = w^[l]a^[l-1] + b^[l]
       w^[l]이 작아지면 z^[l]도 작아지게 된다
       z^[l]이 작아지면 tanh함수로 인해 g(z)는 거의 선형함수가 된다
       모든 레이어가 거의 선형이 되면 전체 네트워크도 거의 선형이 된다

*경사하강법을 구현할 때 비용함수를 경사하강법의 반복 수에 대한 함수로 두면 반복을 진행할 때마다 단조감소하는 함수로 표현할 수 있다.
이를 위해서는 프로베이누스 노름의 합 항까지 모두 포함하여 구현해주어야 한다

## 드롭아웃 정규화
신경망의 각각의 층에 대해 노드를 삭제할 확률을 추가한다
노드 삭제 -> 작고 간소한 네트워크

정규화의 한 방법으로, 네트워크의 과대적합 문제가 발생하기 전까지 드롭아웃을 사용하지 않는다

단점 : J(비용함수)가 더이상 잘 정의되지 않아 비용함수가 잘 하강하는지 확인하기 어렵고, 디버깅도 어렵다
       => 드롭아웃 적용 이후에 코드를 수정하지 않는다

**역드롭아웃**
l = 3,  keep_prob = 0.8
*d3 = np.random.rand(a3.shape[0], a3.shape[1] < keep_prob  #무작위로 행렬 생성
a3.np.multiply(a3, d3)

a3 /= keep_prob
       20%의 확률로 노드 삭제 -> a3가 원래의 20%만큼 감소한다
       이를 해결하기 위해 필요한 20%만큼 채워주어야 하는데 keep_prob로 나누면 그만큼을 채워준다
       이를 통해 스케일링 문제를 해결하고, 테스트 단계에서 드롭아웃을 하지 않아도 활성화 기댓값의 크기가 변화하지 않는다*

test단계에서는 드롭아웃을 사용하지 않는다(노드를 삭제하지 않는다)
       결과가 무작위로 나오기를 바라지 않기 때문

드롭아웃을 하면 입력을 무작위로 삭제하기 때문에 어떤 특성에도 의존할 수 없다. (입력 값이 무작위로 바뀔 수 있기 때문)
=> 특정 입력에 큰 가중치를 주지 않는다
=> 각각 입력에 가중치를 분산시킨다
=> 가중치 노름의 제곱값 감소
=> 과대적합을 막는다(L2정규화와 비슷한 효과가 발생한다)

각 층마다 가중치를 다르게 적용할 수 있다

## 그 밖의 정규화방법
### 훈련세트를 늘린다
훈련세트를 추가적으로 얻을 수 없는 경우인 경우, 혹은 적은 비용으로 추가적인 데이터를 얻기 위해
**사진을 수평방향으로 반전시키거나 무작위로 편집해 새로운 샘플을 만든다**
중복되는 샘플이 많아져 좋지 않지만, 훈련세트가 늘어나는 효과가 있다

**조기종료**
경사하강법을 하게 되면, 훈련오차를 그리거나 최적화하는 비용함수 J를 그리게 된다
이와 함께 개발오차도 그린다
-> 훈련오차와 개발세트오차 사이에서 신경망이 가장 잘 작동하는 순간을 고른다
신경망을 적게 반복할수록 w는 0에 가깝고, 오랜시간동안 훈련하면서 w의 값이 커진다
=> w에 대해 더 작은 노름을 갖는 신경망을 선택하게 된다
       장점 : 경사하강법 과정을 한 번 시도함으로 큰w, 중간w, 작은w를 구할 수 있다
       단점 : ML과정은 여러 과정으로 구성되어 있는데, 그 중 1. 비용함수 최적화 알고리즘 -> 2. 과대적합 문제 해결 의 과정에서 조기종료는 1과 2의 작업이 섞여버린다
              결국 J의 최적화 값을 잘 구하지 못하면서 과대적합을 막게 되어 문제를 더 복잡하게 만들게 된다
              => 이를 해결하기 위해 L2정규화를 사용할 수 있다

## 다중클래스 분류 softmax
softmax : 신경망의 최종 층에서 z^[L]을 게산한다
클래스 C가 여러 개인 경우 사용
특이한 점 : 입, 출력값이 모두 벡터, 클래스 사이의 관계가 선형

Z^[L] = [[5],[2],[-1],[3]]인 경우 이의 활성화 함수를 사용하기 전에 새로운 변수 t를 이용
t  = [[e^5],[e^2],[e^-1],[e^3]]
=> a^[L] = e^z^[L] / S(j=1부터 4까지) ti   -> (4, 1)벡터
a^[t]i = ti/S(j=1부터 4까지)ti
g^[L](z^[L]) = [[e^5/e^5+e^2+e^-1+e^3],[e^2/e^5+e^2+e^-1+e^3],[e^-1/e^5+e^2+e^-1+e^3],[e^3/e^5+e^2+e^-1+e^3]]

C=2인경우 로지스틱 회귀를 이용하는 것과 같다

신경망 학습 시 손실함수
L(y_hat, y) = -S(j=1부터 4까지)yi * log(y_hatj)
위를 최소화해야 함, y_hat의 확률 최대화
비용함수
J(w^[1], b^[1], ...) = 1/mS(i=1부터 m까지)L(yhat^(i), y(i))
경사하강법을 통해 

## CNN합성곱 신경망
**신경망 하위층 -> 모서리 감지 -> 가능성있는 물체 감지 -> 온전한 물체 감지**

### 모서리 감지
1. 수직 이미지 찾기
[필터]
1 0 -1
1 0 -1
1 0 -1

2. 수평 이미지 찾기
[필터]
1   1   1
0   0   0
-1 -1  -1

필터 혹은 커널을 이용해 기존의 이미지를 다른 이미지로 만든다
양과 음의 윤곽선 차이(밝기변화)를 통해 구분한다

소벨 필터
[필터]
1 0 -1
2 0 -2
1 0 -1
=> 중간 부분의 픽셀에 중점을 둔다-> 선명하다

Scharr필터
[필터]
3  0  -3
10 0 -10
3  0  -3

스스로 필터를 학습하도록
[필터]
w1 w2 w3
w4 w5 w6
w7 w8 w9
=> 역전달 형식으로 9개의 변수를 학습하도록 한다 -> 기존의 검출기보다 더 성능이 좋은 윤곽성 검출기가 만들어질 수 있고, 가로/세로/기울어진 윤곽선을 모두 검출할 수 있는 검출기가 만들어질 수 있다

### 패딩
가장자리의 픽셀의 경우 결과 이미지를 만들 때 1회 혹은 중간의 픽셀과 비교하여 적게 사용하고, 이는 결과 이미지로 가장자리 근처의 정보가 전달되지 않는다는 문제점이 존재한다
또한, 기존의 이미지보다 크기가 감소하게 된다
=> 이를 해결하기 위해 합성곱을 하기 이전에 이미지를 덧대는 과정을 수행한다
**이미지 덧대기**
이미지의 가장자리에 보통 0 값을 추가한다
기존의 이미지가 6x6사이즈였다면 이 과정을 통해 8x8이 된다
이 이미지에서 필터를 이용하면 6x6의 이미지를 결과로 얻을 수 있다

유효합성곱 : 패딩을 사용하지 않는다 p=0
             n x n 사이즈의 이미지 * f x f 필터 -> n-f+1 x n-f+1 사이즈의 결과 이미지
동일합성곱 : 패딩을 사용한다
             n x n 사이즈의 이미지 + 패딩 * f x f 필터 -> n+2p-f+1 x n+2p-f+1 사이즈의 결과 이미지
             결과이미지가 input과 사이즈가 동일하기 위해서는 n+2p-f+1 = n 이라는 식을 풀어서 나오는 p값을 이용하면 된다
             p = (f-1) / 2

### 스트라이드
합성곱 신경망의 기본 요소
input이미지를 필터가 몇칸씩 이동할지를 결정하는 하이퍼파라미터

### 3D이미지
2D 이미지가 여러 층으로 겹쳐있다고 생각하면 되고, 2D와 동일하게 수행하면 된다
input 이미지와 필터의 차원은 같아야 한다
ex) 6 X 6 X **3** * 3 x 3 x **3** = 4 x 4

여러개의 필터를 사용하는 경우 결과 이미지의 차원은 사용한 필터의 수와 같아진다
ex) 필터를 2개 사용하는 경우 결과 이미지의 값 : 4 x 4 x 2

### 변수의 수
3 x 3 필터가 10개인 경우 변수의 수 : 
       1개의 필터의 변수의 수 : w -> 3 * 3 * 3 = 27    + b -> 1      => 28개
       10개의 필터의 변수의 수 : 28 * 10 = 280
       이미지의 크기와 상관 없이 280개의 변수로 서로 다른 10개의 특성 분석이 가능
 
---
       l : 합성곱 계층
       f^[l] : 필터 사이즈
       p^[l] : 패딩의 양
       s^[l] : 스트라이드
       input : n_H^[l-1] x n_W^[l-1] x n_C^[l-1](채널 수, 이전 신경망의 필터 수)
       output : n_H^[l] x n_W^[l] x n_C^[l](필터 수)
       n_H^[l] = (n_H^[l-1] + 2p^[l] -f^[l]) / s^[l] + 1의 내림
       n_W^[l] = (n_W^[l-1] + 2p^[l] -f^[l]) / s^[l] + 1의 내림
       활성값 a^[l] = n_H^[l] x n_W^[l] x n_C^[l]
              A^[l] = m x n_H^[l] x n_W^[l] x n_C^[l]
       가중치 w = f^[l] x f^[l] x n_C^[l-1] x n_C^[l](필터를 전부 모은 것, 계층 l의 필터 수)
       bias : 필터마다 하나의 실수값인 편향을 가진다
              n_C^[l] : (1, 1, 1, n_C^[l])
       
---

### 대부분의 신경망 구성
합성곱 층(CONV)
Pooling 층 (POOL)
완전 연결층 (FC)

### 풀링
### 맥스 풀링
필터 내부에서 가장 큰 값만 결과 벡터에 들어간다
특성이 한 곳에서 발견되면 그것을 최대풀링의 결과로 나타낸다
특성이 검출되면 높은 수를 남기게 되고, 특성이 검출되지 않으면 최대값은 여전히 작은 수로 남는다
### 평균 풀링
맥스 풀링과 비슷하지만, 필터 안에서 가장 큰 수만 남기는 맥스 풀링과 달리, 평균 풀링은 필터 내부의 수의 평균을 결과로 나타낸다

**일반적으로 평균풀링보다 최대풀링이 더 많이 사용된다. 7 x 7 x 1000을 1 x 1 x 1000으로 만들 때 평균 풀링이 사용된다
풀링은 파라미터가 없고, 하이퍼 파라미터만 존재한다**

**하이퍼 파라미터**
f : 필터 사이즈
s : 스트라이드
max 또는 average Pooling
p : 패딩의 크기 : 거의 사용하지 않는다, 일반적으로 p=0

일반적인 사이즈
n_H x n_W x n_C = (n_H - f) / s + 1 의 내림 x (n_H - f) / s + 1 의 내림 x n_C

하이퍼파라미터 선택 방법
       직접 선정하는 것보다 다른 사람의 파라미터 선정을 바탕으로 자신의 프로그램에서 잘 작동할 것 같은 파라미터를 선택한다

input이미지 -> CONV1 -> POOL1 -> CONV2 -> POOL2 -> FC3 -> FC4 -> softmax -> 최종값
              하나의 레이어(1)    하나의 레이어(2)
     특징 : 신경망이 깊어질수록 높이와 넒이가 줄어들고, 채널 수는 늘어난다
            CONV-POOL 레이어 -> FC -> softmax -> 최종값의 형식으로 신경망이 구성된다
            
### 합성곱 층 사용 이점
1. 변수 공유 : 벡터의 서로 다른 부분에서 별개의 속성 검출기를 학습할 필요가 없다
2. 희소 연결 : 결과 이미지를 만드는 과정에서 필터 내부에 값들만 결과값에 영향을 미치고, 나머지 픽셀들은 영향을 미치지 않는다
=> 변수를 줄이고, 훈련세트를 줄이는 것이 가능하고, 과대적합도 방지할 수 있다
